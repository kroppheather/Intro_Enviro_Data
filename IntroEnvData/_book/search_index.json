[["introduction.html", "Introduction to Environmental Data and R Chapter 1 Introduction 1.1 Course Learning objectives 1.2 License", " Introduction to Environmental Data and R Heather Kropp 2023 Chapter 1 Introduction Data plays a critical role in addressing major environmental issues related to climate change, conservation, and water security. We gain a critical understanding of the role of observation, the scientific method, environmental sensors, and citizen science in describing the environment. Topics will include data life cycles, statistics, visualization, and statistical programming for environmental data. This book mainly focuses on a basic introduction to the tools used to analyze and interpret giving common types of data associated with the environment. In these tutorials, you will learn about types of data that are core to many environmental topics. Much of the data are observations that directly impact carbon, water, and nutrient cycling. You will work with real world data and problems. You will practice learning data basics and statistics on the following types of data: weather and climate observations greenhouse gas fluxes in ecosystems species abundance and biodiversity data sourced from citizen science *soil observations that are critical for the water cycle, agriculture, and plant health Weather and soil micrometereological measurements are collected every 15 minutes from sensors. An abundance of data used to address environmental issues and research necessitates coding-based, reproducible tools. 1.1 Course Learning objectives Gain an understanding of environmental data Apply statistical analysis and critically interpret data Use code-based, reproducible computational tools 1.2 License Introduction to Environmental Data and Statistics Heather Kropp. Introduction to Environmental Data and Statistics is licensed under a Creative Commons Attribution-NoDerivs license CC BY-ND. "],["introduction-to-r-statistical-software.html", "Chapter 2 Introduction to R Statistical Software 2.1 Learning objectives 2.2 New functions &amp; syntax 2.3 First some terms: 2.4 Getting to know Rstudio Cloud 2.5 Introduction to R &amp; RStudio 2.6 R basics", " Chapter 2 Introduction to R Statistical Software by Heather Kropp for ENVST 206: Introduction to Environmental Data Hamilton College 2.1 Learning objectives Learn about RStudio Cloud Introduction to R &amp; Rstudio Learn the basics 2.2 New functions &amp; syntax c, data.frame,[] $ 2.3 First some terms: R: a statistical programming software that can be used to analyze data RStudio: a user friendly interface for running R Cloud computing a remote computer that you access using your browser. RStudio Cloud runs RStudio in a cloud computing environment. Each assignment or activity you do will open up in this virtual computer. 2.4 Getting to know Rstudio Cloud RStudio Cloud helps create a standardized computing environment for us as a class that is specifically meant to support working in R. This means that you can access the same computing environment and you only need to run a web browser on a personal or Hamilton computer. This will allow coursework to be readily completed from any computer whether you are on or off campus. It also means that you don’t have to worry about installing R or storing data on your own personal computer. You will have received an invitation to join the ENVST206-F21 workspace on RStudio Cloud through your Hamilton email. Once you have joined the workspace, you can access the projects section. A project is a collection of related R files and data. Assignments are projects that are set up to be read only and you will make your own copy when you open it. Your copy will automatically be set up with your user profile once you save it. You will see a blue box next to the project name indicating that it is an assignment. You will have the option to start the assignment for the first time or continue if you already started it. Once you open the assignment, you will see an interface that looks something like this: The Rstudio interface has 4 main components: 1. script window, 2. console, 3. environment, 4. viewer &amp; information tabs. In the next section, you will learn more about each of these parts of the interface. 2.5 Introduction to R &amp; RStudio R is essentially a giant calculator with many options for plotting and built in functions for data analysis. The console runs your R code. It’s the calculator! You can type code into the console and it will run. However, you won’t be able to access that code later, just like many calculators. That’s why we use scripts. R scripts allow you to save code in a text file to run in R. R scripts all have a .r extension. By itself, the script will do nothing. You need to actually tell R to run your code in the console. When you first open a Rstudio, there is often a blank work space with just the console unless I have set up some existing files to open for you. You can tell you are looking at the console by checking the tabs, and you will see information about your R version at the start. New scripts can be created by going to New File and clicking on Rscript. If you wanted to open an existing Rscript, you can go to Open File. Clicking a new script will prompt you to save into the file system created in our project computer: You will see that saving your script will create a new file in the Files tab in the bottom right corner. This tab will be important when we start working with more data files. The script opens up above the console and is ready for you to code! You can get a better idea for the script/console set up by writing a simple line of code in the script and running it. Using the function print, any text within the function will be returned in the console. You can also use hashes # to make a comment so that you can document code. Let’s take a look at how this works before getting into the details. Here is the code in my script: Nothing happens with this code until I run it in the console.You can run a line of code clicking in the line or a selection of code by highlighting it and pressing the run button (shown in the black circle). There’s a few things to pay attention to in the console when you run code. The &gt; symbol always indicates a new line of code. The + symbol means your line of code has not ended and will continue on to the next line. Your results will have numbers in brackets to describe the output. [1] indicates the start of the output. Any variables run in the console are part of your working environment. Variables allow you to refer to the same data or calculations throughout your current R session the computer’s temporary memory. You’ll learn more about R code in the next section to fully understand the utility of the working environment. 2.6 R basics 2.6.1 A fancy calculator Since R is just like a sophisticated calculator, you can read in numerical operations and will get the calculations as output. Type a few different operations like the one below that follow typical mathematical notation on a keyboard. Note I’ve included both the code (grey boxes) and outputs (blue boxes) here for an example. # remember this is a comment so it won&#39;t do anything. R just ignores it. # you need to use these to document code and write notes # 6 raised to the 6 power 6^6 [1] 46656 # 5 plus 210 5+210 [1] 215 # 3 minus 10 3-10 [1] -7 2.6.2 Variables Typing in calculations can quickly become redundant and difficult when you have many observations. Creating a variable allows you to refer to the same object by typing its name. You can create a variable by first typing a name, then typing the assigner &lt;- (= also works, but is not R convention). Anything to the right of the assigner will be refered to with the name you give. Below is an example where I know I will want to use the number 244435600 many times so I will give it a shorter, easier to remember name. # name my number aNumber &lt;- 244435600 # multiply my number by 5 5*aNumber [1] 1222178000 # divide my number by 2 aNumber/2 [1] 122217800 2.6.3 Vectors A vector is a one dimensional array of data. You can make a vector in R using the function c(). c stands for combine values into a vector where each value is separated by a comma. For example, here’s a vector with the elevation of the three highest peaks in the Adirondacks. # make a vector of numbers # elevation in ft peaks &lt;- c(5344,5114,4960) You will notice that both functions print and c are text followed by parentheses. This is a detail to pay attention to for the R syntax. Syntax refers to the rulues and structure of a coding language. What you learned about # and &lt;- are examples of R syntax since these symbols have a specific meaning. Any time you see the format anyName() you can know this is a function. Functions are a key part of R. You already used functions by runnidn c and print. They expect certain inputs called arguments (like our numbers in the c function used to create peaks vector). Functions will also perform a task that saves you a lot of extra coding. You can immediately see the utility of functions by running summary statistics functions like: mean (average value of a vector), min (minimum value in a vector), max (maximum value in a vector). # calculate the mean of peaks mean(peaks) [1] 5139.333 # calculate the minimum peak min(peaks) [1] 4960 # save the maximum as a variable maxPeak &lt;- max(peaks) When you create a variable like maxPeak there is no output shown in the console. It can look alarming, but it is actually a good sign! It means your code worked! The variable maxPeak was simply created. You can always run the name of variable with nothing else to get a print out: maxPeak [1] 5344 If you look to your environment section of RStudio, you will see peaks and maxPeak are shown in the global environment. peaks is a data type numeric and it has 3 total objects. Numeric data are all numbers and can include numbers on both sides of the decimal. This is where watching your environment section is useful to check that your code is working as you expect. You can now do calculations on each object in your vector. For example, if you want to convert the peak elevation to meters, you simply need to type in one calculation. You’ll notice, this calculation was not assigned as a named variable, and a vector of output is returned but not stored in the environment. # convert to meters peaks/3.281 [1] 1628.772 1558.671 1511.734 You can also apply vector operations on vectors that are the same length and a calculation for the element in each vector makes sense. For example, I can make a vector of the prominence (height from the base) and calculate the difference to find the difference between the elevation of the mountain and the prominence. # prominence in ft prom &lt;- c(4914,2100,840) # difference between height and prominence peaks - prom [1] 430 3014 4120 You can also make vectors with other types of data beyond numeric. R supports data types such as dates, character strings, and integers. A character is any mixture of letters, numbers, and symbols. You can’t apply mathematical calculations to character. You can set up a vector of the mountain names by using quotes around each character element in the vector: # quotes denote a character data type peakNames &lt;- c(&quot;Mount Marcy&quot;, &quot;Algonquin Peak&quot;, &quot;Mount Haystack&quot;) You can subset a vector using [] syntax. If you want to refer to just the first item in a vector, you can subset as follows: # elevation of first peak peaks[1] [1] 5344 2.6.4 Data frames The final basic type of data that will be useful to keep in mind is a data frame. Data frames are a matrix with column names and sometimes row names. All observations in a row are associated. Each column will have the same type of data. For example, you can make a dataframe with all of the high peaks information using the ‘data.frame’ function. For the data.frame arguments, you specify each vector to include as a column and the name of the column (left side of each equal sign). # make a datframee # you must include the column name = data vector # seperating multiple columns with commas highPeaks &lt;- data.frame(elev = peaks, prom = prom, name = peakNames) A helpful way to check that your code ran as expected is to track the objects in your global environment. Vectors show up under values and data frames are shown under data. If you click the blue arrow button you will get a preview of the data frame. You can subset a data frame using the [] syntax, but you need to account for the two dimensional nature of data frames. If you are taking all observations for a row or column, it can simply be left empty. You can also refer to columns in a data frame in this format: dataframe$columnName: # subset 2 row in highPeaks highPeaks[2,] elev prom name 2 5114 2100 Algonquin Peak # view only the names column highPeaks[,3] [1] &quot;Mount Marcy&quot; &quot;Algonquin Peak&quot; &quot;Mount Haystack&quot; # look at elevation for 3rd highest mountain highPeaks[1,3] [1] &quot;Mount Marcy&quot; # refer to a column in a data frame using this notation highPeaks$elev [1] 5344 5114 4960 "],["is-this-normal-evaluating-historical-weather-data-to-understand-extremes-changes-and-climate..html", "Chapter 3 Is this normal? Evaluating historical weather data to understand extremes, changes, and climate. 3.1 Learning objectives 3.2 New functions &amp; syntax 3.3 Section 1: The problem. How weird was yesterday’s heatwave? Using long-term weather data to understand normals and climate. 3.4 Section 2: The Data. 3.5 Using packages in R 3.6 Section 3a: The approach. Summary statistics and visualizing data distributions. 3.7 Section 3b: Probability distributions 3.8 The normal probability distribution 3.9 Other distributions 3.10 Citations", " Chapter 3 Is this normal? Evaluating historical weather data to understand extremes, changes, and climate. by Heather Kropp for ENVST 206: Introduction to Environmental Data Hamilton College 3.1 Learning objectives Work with weather data and summary statistics Use histograms to characterize data Probability distributions Characterize climate using a probability distribution 3.2 New functions &amp; syntax read.csv,%&gt;%, filter, summarise, group_by, %&gt;%, histogram,dnorm, pnorm, qnorm 3.3 Section 1: The problem. How weird was yesterday’s heatwave? Using long-term weather data to understand normals and climate. You have probably heard the phrase the “new normal” used to describe warmer temperatures, heatwaves, flooding, and drought that is driven by climate change. This phrase is often used in association with the steadily increasing global mean temperature and higher variability in temperature and precipitation under climate change. In this activity, you will explore how we determine what makes for “normal” weather and how the “new normal” is both literal and figurative in a shifting climate. First some terms: climate refers to the long term weather patterns for a place. It is summarized by averaging observations over many decades. Weather refers to the meteorological conditions over short periods (hourly, daily, annual). On a global scale, the NASA Goddard Institute of Space Studies has been tracking daily land surface temperature anomalies since the 1950s on a global scale to track climate change. A calculation called an anomaly, is used to characterize the difference between an observed value and a mean or baseline value(Anomaly = Observed - Mean). Anomalies are often used to assess temperature and precipitation in a year compare to the typical climatic conditions. The distribution of global annual temperature anomalies observed during each decade are shown in the graph below. A distribution portrays the frequency or rate of occurrence for different observed values. In the graph below, a higher white line and fill color indicates more observations for a particular temperature anomaly value. The x axis value of zero coincides with the mean land surface temperature in 1950. A value of one means the temperature is 1 \\(^\\circ\\) C above the baseline/average. The decadal distributions of land surface air temperatures shown in the graph below help illustrate the increasing mean temperatures and the more frequent occurrence of warmer temperatures. Image source: NASA GISS, Scientific visualization Studio However, not all areas of the globe are changing uniformally. Some areas have experienced accelerated change. The Arctic is now warming 4 times faster than the rest of the globe (Rantanen et al. 2022). In contrast, other areas have experienced minimal change compared to the global average. In order to understand local changes in climate, scientists use long-term historical weather data with complete records (think daily temperature and precipitation for at least 5-8 continuous decades) to examine annual and decadal changes in weather. The average annual air temperature averages daily observations over a year and helps track if a year is warmer or cooler than previous years. The Mean Annual Air Temperature (MAAT or MAT) refers to the mean of many years of annual air temperature values (usually at least 20 years). Precipitation includes rainfall and frozen forms of precipitation like snow and sleet melted down to a liquid (expressed in units of depth of water, mm or inches). Precipitation amounts vary a lot day to day, but summing up the precipitation from every day over the year yields Annual Precipitation. Mean Annual Precipitation (MAP) refers to a long term average of annual precipitation over many years (at least 20 years). Since MAP and MAAT are taken over many years, they represent the general climate of an area. These two variables are often key determinants of the biome covering the land surface(types of vegetation and ecosystems). For example, warm areas with low annual precipitation are considered desert regions. An ecologist, Robert Whittaker, mapped out the biomes that result from different MAAT and MAP combinations across the globe. Image source: Jcraine CC3 3.3.1 Using summary summary statistics to summarize weather Summary statistics help describe data and compare observations. The mean is a measure of central tendency of our data. This often means calculating the average of our data. You have probably calculated it at some point by adding all of you observations (x) and dividing by the total number of observations (n): \\[\\frac{\\sum_{i = 1}^{n}{x_i}}{n}\\] In statistics, there is careful phrasing around the mean versus average, and mean may be used more specifically for a type of statistical concept called a probability distribution (more on this soon!) or when the data more broadly represents a population. In R, the function mean() calls the function for calculating an average so you will have to read into the nuance of mean vs average in each situation. We’ll discuss this more at a later point. There are other ways to summarize the data. You can find the maximum using the max() function or the minimum using the min() function. You often will want to more than the typical value or the highest/lowest values in the data. Measures of the spread of the data are useful for understanding variation in the observations. Measures of spread like the standard deviation help capture how the data observations are spread out around the mean value. For example, the standard deviation of air temperatures in a year tells us about how much day to day variability there is around the annual average. The equation for calculating the standard deviation for a sample of data is: \\[\\frac{\\sum_{i = 1}^{n}{\\sqrt{(x_i - \\bar{x})^2}}}{n-1}\\] Another statistic includes the median. This is the value in the middle of the data (R: median). This means that 50% of the observations are below the number and 50% are above. Often when you hear about record breaking temperatures in heat waves or cold snaps, these observations refer to a value of daily temperature maximum or minimums (highest and lowest temperature in a day) not previously observed in the weather record. Statistics can be used to describe how often we expect to observe these types of events or typical ranges of temperature in a day or year. For example, the amount a temperature or precipitation amount differs from the long term average can be described using the standard deviation as a measure of the distance from the mean (e.g. the value was 2 standard deviations from the mean). Weather events that fall out of typical occurrences of conditions are often classified as extreme weather events. For example, NOAA defines extreme temperatures or rain events as a value that occurs less than 10% of the time. NOAA provides climate graphs for areas with long term weather records that compare weather to climatic normal. Here the daily data is compared to the average daily temperature and the average total precipitation as the year progresses. Such observations help make predictions and decisions related to whether a year is on track to be wetter or drier than usual or warmer/colder. 3.4 Section 2: The Data. In this tutorial and your homework, you will analyze weather data to get familiar with summary statistics, distributions, and data tables with many observations in R. The data that you will work with includes long-term data collected by the National Oceanic and Atmospheric Administration (NOAA) at six weather stations around the United States. NOAA has been observing weather for decades including measurements such as wind, air temperature, and precipitation. Below is an image of NOAA observing stations both past and current: Image Source: NOAA COOP The minimum and maximum daily air temperature and total precipitation are in the data. Air temperature is collected by a thermocouple in a special housing that prevents interference from wind and snow. Precipitation is measured using a bucket with a sensor that measures the amount of water coming in. In the winter, these buckets can be heated to measure liquid water in snow or sleet. The observations span decades. You will learn how to use R and basic statistics to summarize climate from long-term weather data in this tutorial. 3.5 Using packages in R We have been learning to make plots using the built in functions in R including mean and max. While working with these basic plotting functions is useful, it can take a lot more coding to make a nicer looking figure. Packages add additional functions that are not located in the base functions in R. There are a total of 18,088 packages currently available on CRAN that expand the functions available in R. If you were to automatically have all of these packages and their functions loaded in R, it would take up space and too many functions would have the same name. This is why you should load only the packages needed to run the code in your script in an R session. The first step in using a package, is to install it to your local computer using the install.packages function. You only need to do run this once for a project in RStudio Cloud. You will get a prompt asking what mirror you would like to download from. You may choose any mirror. Let’s read in a package that helps with organizing data called dplyr. install.packages(&quot;dplyr&quot;) Once the package is installed, you will need to load the package into your current session using the library function. You will run this every time you start a new R session. library(dplyr) Attaching package: &#39;dplyr&#39; The following object is masked from &#39;package:imager&#39;: where The following objects are masked from &#39;package:stats&#39;: filter, lag The following objects are masked from &#39;package:base&#39;: intersect, setdiff, setequal, union 3.5.1 Reading in data You will read in data using the read.csv function. A csv file (comma separated values) contains the text version of an excel spreadsheet where the values for each cell are included in the file and the designation of a new cell starts with a comma. You can always resave an .xlsx file as a .csv. You can save any excel or sheets file as a .csv file when you click save as. Let’s read in a file I’ve already formatted for you. There is one critical argument in read.csv, the file path including the name of the file. A file path tells R where to find the file within the computer file system. You will be able to see the file in the Files tab. The file system for RStudio cloud will always start with Cloud. You can see the file system structure written out in the red circle: All of the files for this project are on the virtual computer under the file path: /cloud/project. The file is actually saved in the /noaa_weather folder. If you click on the folder, you will see the file path update, and you will see two files. The csv file is the one that you will want to read in. This means that you will read in the file using the entire file path as follows: # read in data # cloud is always lowercase datW &lt;- read.csv(&quot;/cloud/project/noaa_weather/weather_data.csv&quot;) You will see datW appear in your global environment when you run the script. If you click on the blue button, a preview will show you the column names, data type, and the first few rows of data in each column. If you click on the name, datW, a viewer window will appear in the script window. Viewing a data frame allows you to scroll through the rows and columns. However, you cannot actively change or edit the data frame. Information about the data is called metadata. The metadata provides key information about the data, such as the units. Note that I selected the metric option in downloading the data. All missing data fields are indicated with a NA in R formatting. Below is the description for each column name from NOAA: Before you move on, average daily temperature is often more helpful to evaluate temperature than maximum and minimum. The average daily temperature is usually halfway between the minimum and maximum so you can calculate it from the NOAA data. We can calculate it as follows: # calculate the average daily temperature # This temperature will be halfway between the minimum and maximum temperature datW$TAVE &lt;- datW$TMIN + ((datW$TMAX-datW$TMIN)/2) Notice how you were able to type in a single calculation and it was applied to all rows of TMIN and TMAX in datW. For this calculation, I created a new column in datW called TAVE. Remember the convention for referring to a column in a data frame is always dataframe$column. 3.5.2 Subsetting Also note that we have data from six sites with very different climates. You will want to describe annual patterns for each site separately. Otherwise you would be summarizing average conditions over six very different locations around the US, and that doesn’t offer a lot of meaningful interpretation. Here, you want to subset data based on a condition. The use of relational operators allows you to identify data that meets a condition such as (temperature LESS THAN 8 \\(^\\circ\\) C). Logical operators allow you to combine relational statements such as (temperature &lt; 8 °C AND temperature &gt; 0 °C ). Operator Interpretation Type == equal to relational != not equal to relational &gt; more than (not including) relational &gt;= more than or equal to relational &lt; less than (not including) relational &lt;= less than or equal to relational &amp; and logical | or logical A helpful function for finding out all of the station names is the unique function that gives all unique values without repeating them. You can refer to a single column in a data frame using the notation: data.frame$column.name: #get station names unique(datW$NAME) [1] &quot;SYRACUSE HANCOCK INTERNATIONAL AIRPORT, NY US&quot; [2] &quot;STILLWATER RESERVOIR, NY US&quot; [3] &quot;BARROW AIRPORT, AK US&quot; [4] &quot;FARGO HECTOR INTERNATIONAL AIRPORT, ND US&quot; [5] &quot;ABERDEEN, WA US&quot; [6] &quot;PHOENIX AIRPORT, AZ US&quot; You can subset to a single site by using the filter function in dplyr. dplyr allows you to use a type of syntax called a pipe. A pipe will chain functions and data frames together. The pipe is indicated with %&gt;%. When a data frame is put before a pipe, any function run next in the pipe automatically refers to columns in the data frame. This saves you from typing in data.frame$column base formatting. Using the filter function allows you to subset data based on a relational statement. Any critera for which the statement is TRUE will be included in the output. # subset the data to a data frame with just Aberdeen aberdeenDaily &lt;- datW %&gt;% filter(NAME == &quot;ABERDEEN, WA US&quot;) # look at the mean maximum temperature for Aberdeen mean(aberdeenDaily$TMAX) [1] NA You get a NA value here. That’s because there is missing data in this data set. NA is a specification that allows you to know that the data is missing and we should not expect a value. NA is handled differently in R and is neither a number nor character. Luckily there is an argument in mean that allows us to ignore NAs in the calculation. # look at the mean maximum temperature for Aberdeen # with na.rm argument set to true to ingnore NA mean(aberdeenDaily$TMAX, na.rm=TRUE) [1] 14.6133 You will also want to calculate the standard deviation with the function sd. This measures the spread of the observations around the mean, and is in the same units as the mean. # next look at the standard deviation sd(aberdeenDaily$TMAX, na.rm=TRUE) [1] 5.757165 Now you will see the daily maximum temperature in Aberdeen is 14.6 °C. Since this is the mean across many days over decades of observations, this value indicates the typical maximum daily temperature over a long time period. 3.6 Section 3a: The approach. Summary statistics and visualizing data distributions. The above method of calculating means from the previous is not very efficient. In order to calculate mean annual temperature (MAT), we also must take the average of each annual average not the average across all days of observation (confusing, right?). We would have to calculate 71 years of averages for six locations, and that is way too much coding to do each calculation in its own line of code! However, you can use the summarise function to calculate means across an groups. For example This means that we want to treat NAME and YEAR as a grouping variable, and we want to treat each station as a different group. A group indicates that each unique value in the group describes a homogeneous feature (e.g. landcover, day of year). The group_by function in dplyr can be used to specify a grouping variable. summarise creates a new data frame with column labels for each group and any columns created in the function. # get the mean, standard deviation across all sites and years averageTemp &lt;- datW %&gt;% # all data will be in datW group_by(NAME, YEAR) %&gt;% # NAME and YEAR as groups summarise(TAVE=mean(TAVE, na.rm=TRUE), # calculate average for each station x year TAVE.sd = sd(TAVE, na.rm=TRUE)) # standard deviation `summarise()` has grouped output by &#39;NAME&#39;. You can override using the `.groups` argument. If we want to calculate the Mean Annual Temperature (MAT), then we need to average the temperature for each location across all years. This can be done with aggregate again: #calculate the mean annual average temperature for all sites (MAT) MAT &lt;- averageTemp %&gt;% # refer to averageTemp data frame group_by(NAME) %&gt;% # group by station name summarise(MAT = mean(TAVE)) # calculate the mean for each station These average temperature values from decades of temperature can tell you about the overall climatic conditions of the different locations in the data. You can use any function in aggregate. For example, you can replace mean with max to find the annual average temperature value observed in the data. Finally, we will want to explore an individual location in more detail. It will be helpful to create a new data frame that only has the annual average temperature for the location in the Adirondack Park (Stillwater Reservoir). We can create a new data frame by subsetting the averageTemp data frame and assigning it a name: # create a data frame for only Stillwater Reservoir MAT (located in Adirondack park) adk &lt;- averageTemp %&gt;% filter(NAME == &quot;STILLWATER RESERVOIR, NY US&quot;) 3.6.1 Histograms There are over 155,000 daily observations in our data file and each site has over 70 years of data. Summary statistics are helpful, but it is important to visualize the data. A graphical tool called a histogram can help visualize how frequently certain values are observed in the data. This is called the data distribution. A histogram shows the frequency of temperature observations in different bins. The start and end of a bin is called a break in R. The hist function generates a histogram. #make a histogram for Syracuse hist(adk$TAVE,#data freq=TRUE, #show count as discrete number main = &quot;Stillwater Reservoir in ADK Park&quot;, #title of plot xlab = &quot;Average annual temperature (degrees C)&quot;, #x axis label ylab=&quot;Relative frequency&quot;, # yaxis label col=&quot;grey75&quot;, #colors for bars border=&quot;white&quot;) #make the border of bars white You can see that annual average temperature varies between 2-7 °C in at the Stillwater Reservoir. The distribution is also fairly symmetrical with very high and low temperature values occurring at similar rates and most observations in the middle of the data range. To get a better idea of how the summary statistics describe the data, let’s take a closer look at the plot. I’ll add a red solid line for the mean and red dashed lines for the standard deviation from the mean. I’ll add a dotted line to mark the values that are within two standard deviations of the mean. You can see that the mean is fairly close to the center of the distribution and the observations that occur most frequently are within 1 standard deviation of the mean. Most of the data is within 2 standard deviations of the mean. These properties are meaningful statistically, in the next section, you will learn about more formal, mathematical ways to describe data distributions. 3.7 Section 3b: Probability distributions 3.7.1 Normal distributions The data distribution that we just viewed has a very particular shape. The temperature observations are most frequent around the mean and we rarely observe data 2 standard deviations from the mean. The distribution is also symmetrical. We can describe this occurrence of different values of data more formally with a probability distribution. Probability distributions have a lot of mathematical properties that are useful. We use parameters to help describe the shape of the data distribution. This temperature data follows a normal distribution. This type of distribution is very common and relies on two parameters: the mean and standard deviation to describe the data distribution. Let’s take a look at a normal distribution assuming the mean and standard deviation parameters are equal to the ones observed in the Adirondack data: The distribution describes the probability density which is the relative occurrence of all values of data. The probability density is not a probability. The bell curve of the normal distribution follows a very specific shape. In fact, let’s look at a special case of the normal distribution to better understand the shape. Below is an image of the normal distribution where zero represents the value at the mean of the data and tick marks are designated with standard deviations. This is called a standard normal or a z distribution. Data can be converted to this scale using the calculation: (data - mean)/standard deviation Probability distributions all have functions in R. Below, you can see the dnorm function is used to generate the probability density for a range of temperature values in the plot. Remember this probability density alone just gives us the overall shape and relative occurrence, but the density at any given value is not an actual probability. The arguments for dnorm, values to calculate the probability density, the mean, and the standard deviation. You can plot a normal distribution like the one above by making a sequence of numbers (seq) to draw the curve for. A plot will require basic, x and y data. You can always check the documentation of a function using the help() function. Simply run help with the name of a function to find the arguments and default settings in a function. Below is an example for dnorm: help(dnorm) We’ll learn more about plotting soon. For now here are the code basics to plot a normal distribution: # make a sequence of temperature values xseq &lt;- seq(2,#starting number 8,#ending number by=0.1)#increments for making numbers # calculate normal distribution pd xNorm &lt;- dnorm(xseq, mean = mean(adk$TAVE,na.rm=TRUE), sd= sd(adk$TAVE,na.rm=TRUE) ) # make a plot plot(xseq, # x data xNorm, # y data type=&quot;l&quot;,# make a line plot xlab=&quot;Average Annual Temperature&quot;, # label x axis ylab=&quot;Probability density&quot;) # label y axis This distribution can be compared to the data observations by overlaying the two graphs: You can now see the blue dashed line overlain on the histogram of Adirondack TAVE values. This is the normal distribution using the mean and standard deviation calculated from the data. You’ll notice the normal distribution does a good job of modeling our data. Sometimes it underestimates a data bin and other areas are overestimated, but overall it mirrors the distribution of our data. This means we can rely on properties of the normal to help describe our data statistically! We’ll learn a little more in the coming weeks about all of the functionality this offers. 3.8 The normal probability distribution Let’s turn to daily data and think about what an abnormally warm day might entail. For this exercise, let’s look at daily values for Aberdeen, Washington using the aberdeenDaily data frame that you created earlier: # preview the first few lines of aberdeen daily head(aberdeenDaily) STATION NAME LATITUDE LONGITUDE ELEVATION DATE PRCP TMAX 1 USC00450008 ABERDEEN, WA US 46.9658 -123.8291 3 1950-01-01 5.3 5.0 2 USC00450008 ABERDEEN, WA US 46.9658 -123.8291 3 1950-01-02 9.4 5.0 3 USC00450008 ABERDEEN, WA US 46.9658 -123.8291 3 1950-01-03 1.0 -2.2 4 USC00450008 ABERDEEN, WA US 46.9658 -123.8291 3 1950-01-04 9.7 -2.8 5 USC00450008 ABERDEEN, WA US 46.9658 -123.8291 3 1950-01-05 0.0 0.6 6 USC00450008 ABERDEEN, WA US 46.9658 -123.8291 3 1950-01-06 22.1 5.6 TMIN YEAR TAVE 1 -2.2 1950 1.40 2 -7.8 1950 -1.40 3 -7.8 1950 -5.00 4 -7.2 1950 -5.00 5 -3.9 1950 -1.65 6 -1.7 1950 1.95 You can see that the daily values follow a normal distribution. We can use the normal distribution to calculate the probability of different ranges of daily temperature values. For a given value of the data, the normal distribution has a probability density associated with observing the value. The probability density doesn’t mean anything at a given value of the data. However, when the normal distribution is integrated across a range of values, it yields a probability for the occurrence of the range of values. For those of you that haven’t had calculus, integrating is essentially taking the area under the curve between a range of numbers. In my graph below, you can see the red shading indicates the area below the value of zero on the curve. We have to keep in mind that the range of the normal distribution extends from -\\(\\infty\\) to \\(\\infty\\). Let’s start by taking a look at all values below freezing in the normal distribution for our Aberdeen weather data. Technically this is the probability of all temperatures below freezing from zero to -\\(\\infty\\). Functionally we know some low temperatures would be impossible to observe on earth and the probability of observing values closer to -\\(\\infty\\) will be minuscule. You’ll notice that I cut off the axis at -10 and 30 degrees C where the occurrence of values outside of this range is so low, it looks like zero. Luckily we don’t have to do any of the work calculating the probability. R has a built in suite of functions for working with probability distributions. You can run the help command documentation for all functions related to the normal distribution. Run the documentation on dnorm to see them all: help(dnorm) R uses p to designate probability. If we use pnorm, we can enter a number as the first argument (the next two arguments are always the mean and standard deviation for the normal), the output will be the probability of observing that value AND all values below it to -\\(\\infty\\). Let’s calculate the probability of below freezing temperatures. Don’t forget that probabilities always range from 0 to 1. We would have to go integrate across the normal between values of -\\(\\infty\\) to \\(\\infty\\) to get a probability of 1 in the normal. Below we’ll just focus on all values below freezing: #pnorm(value to evaluate at (note this will evaluate for all values and below),mean, standard deviation) pnorm(0, mean(aberdeenDaily$TAVE,na.rm=TRUE), sd(aberdeenDaily$TAVE,na.rm=TRUE)) [1] 0.01617285 You can see temperatures below freezing are rare at this site and we only expect them to occur about 1.6% of the time. Sometimes it’s easier to think about probability as % occurrence by multiplying the probability x 100. You can take advantage of the properties of the distribution and add and subtract areas under the curve to better tailor my ranges of numbers. For example, I might be interested in identifying how often a temperatures between 0-5 degrees occur. First you can find out the probability of all values at 5 degrees or below: #pnrom with 5 gives me all probability (area of the curve) below 5 pnorm(5, mean(aberdeenDaily$TAVE,na.rm=TRUE), sd(aberdeenDaily$TAVE,na.rm=TRUE)) [1] 0.1307616 Next, you can subtract the probability for observing values below 0 from your first probability, and you will get the probability of temperatures in the range of 0-5. #pnrom with 5 gives me all probability (area of the curve) below 5 pnorm(5, mean(aberdeenDaily$TAVE,na.rm=TRUE), sd(aberdeenDaily$TAVE,na.rm=TRUE)) - pnorm(0, mean(aberdeenDaily$TAVE,na.rm=TRUE), sd(aberdeenDaily$TAVE,na.rm=TRUE)) [1] 0.1145887 Now let’s evaluate the probability of high temperatures. Knowing that the entire distribution adds up to 1, you can also find the area above a value by subtracting the probability below that given value from 1. For example, let’s look at the probability of temperatures above 20 degrees C. #pnrom of 20 gives me all probability (area of the curve) below 20 #subtracting from one leaves me with the area above 20 1 - pnorm(20, mean(aberdeenDaily$TAVE,na.rm=TRUE), sd(aberdeenDaily$TAVE,na.rm=TRUE)) [1] 0.02685653 There qnorm function will return the value associated with a probability. This is the value in which all values at or below the value equal that probability. Let’s use this to evaluate extreme weather events. Let’s assume everything that occurs with a probability of less than 10% of the time (either hot or cold so anything above 95% or anything below 5%) is unusual. Let’s examine what unusually high temperatures in Aberdeen start at: #qnorm gives me the value at which all values and below equal the probability in my argument #Here I&#39;m calculating the value of the 95th quantile or a probability of 0.95 qnorm(0.95, mean(aberdeenDaily$TAVE,na.rm=TRUE), sd(aberdeenDaily$TAVE,na.rm=TRUE)) [1] 18.60274 This means I expect 95% of the temperature observations to be below this value. Any temperature observations above this value will occur with a probability of 5%. Note: Throughout all of my code examples, you’ll notice that I continued to copy and paste the same code for calculating the mean for site 1: mean(aberdeenDaily$TAVE,na.rm=TRUE). While I did this to help you remember what was going into the function, it gets confusing and messy in long functions. This is a perfect example of why we name variables (with short clear names!) to refer to later on. As this course progresses, we’ll continue to work on creating clean code once you get more comfortable with R. We can also take advantage of the normal distribution to examine whether the distribution of daily average temperature has changed in recent decades. Let’s look at the first 35 years of data (1950-1985) and the recent years (&gt; 1985): # subset before and equal to 1985 AberdeenPast &lt;- aberdeenDaily %&gt;% filter(YEAR &lt;= 1985) # subset years greater than 1985 AberdeenRecent &lt;- aberdeenDaily %&gt;% filter(YEAR &gt; 1985) You can see that the mean is higher in recent decades, shifting the distribution. We can look at the difference probability in each period. For example, let’s look at the probability of daily temperatures exceeding 18 degrees C: # probability of daily temeprature above 18 degrees in 1950- 1985 1-pnorm(18,mean(AberdeenPast$TAVE,na.rm=TRUE), sd(AberdeenPast$TAVE,na.rm=TRUE)) [1] 0.05969691 # probability of daily temeprature above 18 degrees in 1986- 2021 1-pnorm(18,mean(AberdeenRecent$TAVE,na.rm=TRUE), sd(AberdeenRecent$TAVE,na.rm=TRUE)) [1] 0.06795978 There is a small increase in probability of daily temperatures at or exceeding 18 degrees in recent decades. 3.9 Other distributions This tutorial only addresses the basics of continuous probability density distributions. Discrete data follows slightly different probability rules and should be treated differently. There are many probability distributions, too many to cover all here. Each probability distribution describes different parameters and ranges of data. Let’s look at a few distributions common to environmental data: Exponential The exponential distribution describes data that has the most frequent observations at a value of zero, with a rapid non-linear decline in probability density. The distribution describes data between zero (including) and \\(\\infty\\). There is a single parameter (the rate, \\(\\lambda\\)). The functions dexp, pexp, and qexp can all be used to call the Student’s t distribution The Student’s t distribution is a symmetrical, bell shaped distribution similar to the normal distribution. The main difference is the t distribution has heavier tails. This means that more extreme values further from the center of the distribution occur a little more frequently than the normal. The main parameter is the degrees of freedom, often abbreviated with \\(\\nu\\) or n. The distribution is typically centered at zero, but can be specified to be centered at a different value in R. You can refer to the function in R with dt, pt, and qt. Gamma The Gamma distribution describes data ranging from zero (including) to \\(\\infty\\). The function involves two parameters, the shape (\\(\\alpha\\)) and the rate (\\(\\beta\\)). It can take on a range of forms including asymmetrical distributions and exponential type curves. The function in R ar dgamma, pgamma, and qgamma. Note There are too many distributions to name. Wikipedia has great overviews of different probability density functions, but beware that I have found occasional mistakes such as in the ranges of x values that the function spans. Wolfram Alpha has great, more accurate resources, but it can be a little dense and jump into more theory. You can take an entire class in probability (it’s so cool!), but this class will only cover the basics needed for environmental applications. Great work! You just learned how to work with a lot of weather observations (&gt;155,000)! Imagine trying to calculate 72 averages across 26,117 rows of data in Excel or Sheets. You just did that in one line of code! You also learned to use R to describe climate, summarize data, characterize rare events, and calculate a probability of occurrence. You will continue working on these problems throughout your homework to characterize climate and examine changes in temperature and precipitation in different regions of the United States. 3.10 Citations NASA’s Scientific Visualization Studio. Shifting Distribution of Land Temperature Anomalies, 1951-2020. Accessed 2022. https://svs.gsfc.nasa.gov/4891 Rantanen, Mika, et al. “The Arctic has warmed nearly four times faster than the globe since 1979.” Communications Earth &amp; Environment 3.1 (2022): 1-10. "],["visualizing-data.html", "Chapter 4 Visualizing data 4.1 Learning objectives 4.2 New functions &amp; syntax 4.3 Fundamentals of plotting data 4.4 Plotting data in base R 4.5 Using packages in R 4.6 Plotting in ggplot2 4.7 Exploring different visualizations in ggplot2", " Chapter 4 Visualizing data by Heather Kropp for ENVST 206: Introduction to Environmental Data Hamilton College 4.1 Learning objectives Fundamentals of plotting data Plotting data in base R Learn about different visualization techniques in ggplot2 Characterize climate and weather data graphically 4.2 New functions &amp; syntax n,plot, points, legend,ggplot, geom_point,geom_line,labs 4.3 Fundamentals of plotting data Visualizing data requires careful consideration of the audience and representation of the data. Many aesthetics and graphic design principles can aid in making engaging and accurate visualizations. There are several main principles to consider in visualizing data: 4.3.1 1. Representation of data The representation of data will depend on the type of data (categorical, discrete, proportion, numerical, etc.) and the uncertainty associated with the data. Keep in mind, uncertainty may be related to measurement error or simply showing the variation or spread of the data . Typically the type of data influences the type of graph that can be used. Below are a few examples of the underlying representation of data and associated error or variation. The choice in plot will depend on the representing uncertainty or variation (error bars, quantile boxplot), independence or ordering of variables (line versus scatter), and categorical versus numerical data (barplot, mosaic, pie). 4.3.2 2. Layout The layout describes all of the features and arrangement of the graph including labeling, axes range, the main graph frame. The objects included for both labeling and the physical centering and balance of the elements can all fall under layout. 4.3.3 3. Encoding Encoding deals with the symbolization and representation of data. This can be related to colors, size of points or boxes, line weight, shading, or transparency. For encoding, you will want to consider aspects such as intuitive interpretation, accessibility, and cultural meaning. In the above graph, colors are similar to the objects in the data and the point size represents large/small values. These types of intuitive encoding help with interpretation. However, for a broad audience, color-blind friendly and high contrast color may be more favorable over colors associated with objects or meaning. You will want to choose colors that will avoid confusion with other associations (like red colors for low values). You should also keep in mind that there are limitations in the number of hues that can be readily related to a number or object by most people. This means that encoding data via shading or hue can communicate general patterns for a large range of hue values, but does not offer the more accurate assessment that position or length can convey. 4.3.4 4. Simplicity Too many colors, crowded points, complex shapes, and overlapping text all hamper the interpretation of data. In many contexts, there can be a lot of information to convey in a limited amount of space and audience attention span. Visualization often requires coordinating with text and the venue for the visualization to narrow in on the main focus and key takeaways from the visualization. Effective titles and labels can also play a role in visualization. 4.4 Plotting data in base R Plots are one of the most useful tools for visualizing weather patterns and long term climate trends. We will work with the NOAA weather data from activity 2 to get more comfortable with making plots in R. # read in data # cloud is always lowercase datW &lt;- read.csv(&quot;/cloud/project/noaa_weather/weather_data.csv&quot;) It can be helpful to set up a vector of the station names for easy reference. Working with station names as factors instead of characters often works better in some plotting functions. The as.factor function can convert the data. With this function, you can reassign the vector names as factors instead of characters. The levels function allows you to view each unique station name. # specify that the name column should be a factor datW$NAME&lt;- as.factor(datW$NAME) # set up a vector of all names for each level nameS &lt;- levels(datW$NAME) nameS [1] &quot;ABERDEEN, WA US&quot; [2] &quot;BARROW AIRPORT, AK US&quot; [3] &quot;FARGO HECTOR INTERNATIONAL AIRPORT, ND US&quot; [4] &quot;PHOENIX AIRPORT, AZ US&quot; [5] &quot;STILLWATER RESERVOIR, NY US&quot; [6] &quot;SYRACUSE HANCOCK INTERNATIONAL AIRPORT, NY US&quot; Let’s start by taking a closer look at precipitation at each site. Just as you did in the previous chapter, totaling the precipitation that occurred in a year using the summarise and group_by function and sum will allow you to examine total annual precipitation. However, you want to avoid years with too many missing observations. In most cases involving missing observations, annual precipitation will be biased lower than the actual values of precipitation that fell. You would want to be careful comparing precipitation across sites and years when most of the the year is missing. It can be helpful to remove the NAs to sum the precipitation observations and account for the number of observations with real data. You will want to start by putting precipitation in a separate data frame and omitting NAs using the na.omit function. This will omit any row of data with an NA in it. # remove NA using na.omit precipSub &lt;- data.frame(NAME=datW$NAME, YEAR=datW$YEAR, PRCP=datW$PRCP) datP &lt;- na.omit(precipSub) # total annual precipitation (mm) # use summarise to get total annual precipitation precip &lt;- precipSub %&gt;% group_by(NAME, YEAR) %&gt;% summarise(totalP = sum(PRCP), # sum all precip ncount = n()) #get the number of observations `summarise()` has grouped output by &#39;NAME&#39;. You can override using the `.groups` argument. If you scroll through the ncount column, you’ll see a lot of sites have a full year of observations 365/366, but some sites x years have a very low number of observations such as only 143 days. Let’s remove years with too many missing precipitation measurements. There is not always a clear method to consider what might be acceptable amount of missing data. In this case, let’s allow for one to two days (depending on leap year) of missing data: # make a new dataframe with observations at or above 364 pr &lt;- precip %&gt;% filter(ncount &gt;=364) Now let’s make a plot that looks at precipitation throughout the years. Let’s start by looking at the the town formally known as Barrow, Alaska (now known as Utqiagvik) and the Fargo, North Dakota data. Utqiagvik is a polar desert and Fargo is a cold, continental semi-arid area mostly consisting of prairie habitat. The sites are the second and third levels in our factor data. We can subset the entire data frame: # look at only Utqiagvik (Barrow, AK) and Fargo, ND annual precipitation ak &lt;- pr %&gt;% filter(NAME == nameS[2]) nd &lt;- pr %&gt;% filter(NAME == nameS[3]) Let’s start by looking at the basic plot in R using the Arizona data first using the plot function. The basic arguments involve data for the x axis and data for the y axis. # make a plot of Alaska annual precipitation plot(ak$YEAR, # x data ak$totalP) # y data You’ll notice there are a few aspects of this plot that are difficult to read. There are so many years of observations, and it is difficult to track the year to year changes in precipitation. Adding lines can improve the readability of these sequential observations. It is also a good idea to keep the points since some years may be missing observations. With a few arguments the plot can be improved: # make a plot of AK precipitation plot(ak$YEAR, # x data ak$totalP, # y data type = &quot;b&quot;, #b = both points and lines added pch = 19, # symbol shape to be a filled in circle ylab = &quot;Annual precipitation (mm)&quot;, #y axis label xlab = &quot;Year&quot;) #x axis label You’ll also note that some of the y axis ticks are cut off because the plot window is too small and the labels are flipped in a hard to read direction. R doesn’t offer a lot of axes arguments within the plot function, but we can turn them off and add them separately with the axis function. You can turn all axes off in plot using the axes=FALSE argument or individual axis off. In this case, the x axis looks good so need to change it. yaxt = \"n\" turns off just the y axis. # make a plot of Alaska precip plot(ak$YEAR, ak$totalP, type = &quot;b&quot;, pch = 19, ylab = &quot;Annual precipitation (mm)&quot;, xlab = &quot;Year&quot;, yaxt = &quot;n&quot;) # turn off automatic labels on yaxis # add y axis # arguments are axis number (1 bottom, 2 left, 3 top, 4 right) # las = 2 changes the labels to be read in horizontal direction axis(2, seq(0,400, by=100), las=2 ) The plot is looking much better! Now let’s add Fargo, ND for comparison. The points function adds points to an existing plot. plot(ak$YEAR, ak$totalP, type = &quot;b&quot;, pch = 19, ylab = &quot;Annual precipitation (mm)&quot;, xlab = &quot;Year&quot;, yaxt = &quot;n&quot;) # add y axis axis(2, seq(0,400, by=100), las=2 ) # add Fargo points(nd$YEAR, nd$totalP, # x and y data type = &quot;b&quot;, # points and lines pch = 19, # filled in circle points col=&quot;tomato3&quot;) # change the color Only one observation in North Dakota is within the range of the Alaska site. The rest of the data is not shown because it falls outside of the The axis range will need to be fixed so that all observations are visible for both sites. Here the xlim and ylim arguments change the range of the axes. These arguments expect a vector of two numbers: ylim = c(minimum value, maximum value). This tells R the range on the axis to plot. plot(ak$YEAR, ak$totalP, type = &quot;b&quot;, pch = 19, ylab = &quot;Annual precipitation (mm)&quot;, xlab = &quot;Year&quot;, yaxt = &quot;n&quot;, ylim =c(0, 900)) # change the limits, c(lower, upper) #add y axis axis(2, seq(0,900, by=300), las=2 ) #add Fargo points(nd$YEAR, nd$totalP, type = &quot;b&quot;, pch = 19, col=&quot;tomato3&quot;) There is just a couple of last considerations to finish the plot. You can add a legend to properly label the plot using legend function. The first argument specifies the placement using either the exact x,y coordinate or you use an argument for a more general position such as: topleft, center, bottomright. You could also add a line to indicate where the mean annual precipitation values are for each site using the abline function from tutorial 2. The h= argument adds a horizontal line at a value plot(ak$YEAR, ak$totalP, type = &quot;b&quot;, pch = 19, ylab = &quot;Annual precipitation (mm)&quot;, xlab = &quot;Year&quot;, yaxt = &quot;n&quot;, ylim =c(0, 900)) #add y axis axis(2, seq(0,900, by=300), las=2 ) #add Fargo points(nd$YEAR, nd$totalP, type = &quot;b&quot;, pch = 19, col=&quot;tomato3&quot;) #add legend legend(&quot;topleft&quot;, #position c(&quot;Alaska&quot;, &quot;North Dakota&quot;), #labels col= c(&quot;black&quot;, &quot;tomato3&quot;), #colors pch=19, #point shape lwd=1, #line thickness 1, anytime both point &amp; line arguments are given both will be drawn bty=&quot;n&quot;) #always use this argument otherwise an ugly box is drawn #add lines at the mean annual precipitation for each site abline(h= mean(ak$totalP), # horizontal line at the mean col=&quot;black&quot;) # color set to black abline(h= mean(nd$totalP), # line at nd mean col=&quot;tomato3&quot;) # color set to tomato3 4.5 Using packages in R We have been learning to make plots using the built in functions in R including plot and histogram. While working with these basic plotting functions is useful, it can take a lot more coding to make a nicer looking figure. Packages add additional functions that are not located in the base functions in R. There are a total of 18,088 packages currently available on CRAN that expand the functions available in R. If you were to automatically have all of these packages and their functions loaded in R, it would take up space and too many functions would have the same name. This is why you should load only the packages needed to run the code in your script in an R session. The first step in using a package, is to install it to your local computer using the install.packages function. You only need to do run this once for a project in RStudio Cloud. You will get a prompt asking what mirror you would like to download from. You may choose any mirror. install.packages(&quot;ggplot2&quot;) Once the package is installed, you will need to load the package into your current session using the library function. You will run this every time you start a new R session. library(ggplot2) 4.6 Plotting in ggplot2 ggplot2 contains many different functions to make nice looking plots easily. You’ll notice we needed to run many lines of code to make the axes and points look nice. ggplot2 has developed a different plotting syntax than base R. The language of ggplot2 means that you declare the data that you are using, the coordinate system, and the geometry to be used in the plot. The data refers to the data source and the aesthetics. In the aesthetics you define the x, y coordinate specifications and any other information on the size or colors to be used. The coordinate system specifies how to plot the data graphically. By default it refers to a cartesian coordinate system which simply means plotting on a x,y axis. You don’t need to specify this plotting component if you are making a cartesian graph. The geometry refers to the shapes/type of plot to be used (ex: bars, boxes, points). You can find a cheat sheet for ggplot2 here: https://github.com/rstudio/cheatsheets/blob/master/data-visualization-2.1.pdf Let’s get a feel for using ggplot by looking at the annual precipitation across years for all of the sites. You’ll notice that ggplot2 also uses + signs to connect multiple functions. This is unique to this type of package and other packages made by the same developers. It does not work for functions outside of these packages ggplot(data = pr, aes(x = YEAR, y=totalP, color=NAME ) )+ # data for plot geom_point()+ # make points at data point geom_line()+ # use lines to connect data points labs(x=&quot;year&quot;, y=&quot;Annual Precipitation&quot;) # make axis labels Warning: Removed 11 rows containing missing values (`geom_point()`). Warning: Removed 2 rows containing missing values (`geom_line()`). You’ll notice a legend is automatically made and the y axis labels are in the right direction. There’s just a couple more things. I personally dislike the gray gridlines. A lot of people like them and that’s fine if you like them. Let’s look at how we can get rid of them by changing the theme. ggplot(data = pr, aes(x = YEAR, y=totalP, color=NAME ) )+ # data for plot geom_point()+ # make points at data point geom_line()+ # use lines to connect data points labs(x=&quot;year&quot;, y=&quot;Annual Precipitation&quot;)+ # make axis labels theme_classic() # change plot theme Warning: Removed 11 rows containing missing values (`geom_point()`). Warning: Removed 2 rows containing missing values (`geom_line()`). It also would be helpful to change the colors and make them semi-transparent so we can see what sites overlap a little better. You can manually change the colors in Rstudio using the scale_color_manual setting. Note in R, you can enter colors as color names: such as \"tomato3\", the hex code, or you can use the rgb() function to designate the amount of red, green, and blue that goes into making a color. There are a lot of color resources online that can help you pick colors. If you search for the color tomato3 in google, you will get results from color websites that give the hex codes or the red,green, blue composition. In the case of tomato3, the hex code is #CD4F39 and the red, green, blue composition is rgb(0.8, 0.31,0.22). You can find more information on color theory and colors in R here: https://www.nceas.ucsb.edu/sites/default/files/2020-04/colorPaletteCheatsheet.pdf ggplot(data = pr, aes(x = YEAR, y=totalP, color=NAME ) )+ geom_point(alpha=0.5)+ geom_line(alpha=0.5)+ labs(x=&quot;year&quot;, y=&quot;Annual Precipitation (mm)&quot;)+ theme_classic()+ scale_color_manual(values = c(&quot;#7FB3D5&quot;,&quot;#34495E&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;,&quot;#26A69A&quot;,&quot;#58504A&quot;)) Warning: Removed 11 rows containing missing values (`geom_point()`). Warning: Removed 2 rows containing missing values (`geom_line()`). 4.7 Exploring different visualizations in ggplot2 One of the main considerations in data visualizations is using the proper type of geometry to display data. Let’s look at making different types of plots in ggplot2. Violin plots are a way to display data across categories in a manner that conveys more information than a barplot showing the mean. The violins are created using a mirrored histogram of the data that is smoothed out into a line. Each distribution is displayed around the name with wider parts of the violin representing values on the y axis that occur more frequently within a group. Typically violins are paired with a boxplot in the violin to show the quartiles (25% and 75% of the data ranges), median, and 95% range within the distribution. In ggplot2, violin plots are easy to make. Let’s focus on minimum temperatures for this next part. Let’s take a look at the daily minimum temperatures in Alaska, Washington, and Stillwater Reservoir. # subset so that data can come from AK, Stillwater, or WA plotSub &lt;- datW %&gt;% filter(datW$NAME == nameS[1] | datW$NAME == nameS[2] | datW$NAME == nameS[5]) ggplot(data = plotSub, aes(x=NAME, y=TMIN))+ # look at daily tmin geom_violin(fill=rgb(0.933,0.953,0.98))+ # add a violin plot with blue color geom_boxplot(width=0.2,size=0.25, fill=&quot;grey90&quot;)+ # add grey #boxplots and make them about 20% smaller #than normal with 25% thinner lines than normal theme_classic() # get rid of ugly gridlines You may get a warning message that there were non-finite values. That’s warning you that NA values were omitted. No problem that they were removed so you can ignore that message. Let’s wrap up by looking at one other component of plotting that will be useful. Let’s take a closer look at daily patterns within a year in Syracuse NY in 2020. sub &lt;- datW %&gt;% filter(datW$NAME == nameS[6] &amp; datW$YEAR == 2020) It will be helpful to work with dates on the x-axis now. Right now, R thinks the dates are characters. Let’s change that to a date format. There are many ways to format a date: 01-28-2020 and 01/28/2020 both refer to the same date. You have to specify that you want your data to be treated like a date, but you also have to indicate how it is formatted. as.Date indicates the vector of data should be considered a date. # specify date format # %Y means a four number year # - indicates that the date uses dashes to separate # %m means month # %d means day sub$DATE &lt;- as.Date(sub$DATE,&quot;%Y-%m-%d&quot;) You can find other types of formatting for dates here: https://www.statmethods.net/input/dates.html Now when you made plot with date on the x axis in ggplot, you’ll see nicely labeled date information automatically provided. ggplot(data=sub, aes(x=DATE, y=TMAX))+ geom_point()+ geom_path()+ theme_classic()+ labs(x=&quot;year&quot;, y=&quot;Maximimum temperature (C)&quot;) In one last plot example, lets look at a barplot. Barplots are helpful for precipitation since we are often looking at totals for a day or year and in many cases do not have variability in each value. ggplot(data=sub, aes(x=DATE, y=PRCP))+ geom_col(fill=&quot;royalblue3&quot;)+ theme_classic()+ labs(x=&quot;year&quot;, y=&quot;Daily precipitation (mm)&quot;) "],["introduction-to-hypothesis-testing.html", "Chapter 5 Introduction to hypothesis testing 5.1 Learning objectives 5.2 New functions &amp; syntax 5.3 Introduction to two sample t-tests 5.4 One sample t-test in R 5.5 Introduction to chi-squared test 5.6 Citations", " Chapter 5 Introduction to hypothesis testing by Heather Kropp for ENVST 206: Introduction to Environmental Data Hamilton College 5.1 Learning objectives Learn about data describing biodiversity and species impacts on ecosystems Learn how to conduct statistical hypothesis testing for t-tests and chi squared tests Interpret statistical output in R 5.2 New functions &amp; syntax stat.summary, shapiro.test, bartlett.test,tapply,t.test, chisq.test, mosaicplot 5.3 Introduction to two sample t-tests 5.3.1 The data The consumption of vegetation by herbivores can impact the carbon cycling through by reducing plant biomass. These impacts can even change the energy, water, and nutrient cycling in an area. Brown lemmings are a type of rodent that lives in the Arctic in Alaska and eastern Siberia. An example of a lemming. CC3:Frode Inge Helland Lemming populations fluctuate over time. When lemming populations reach high numbers in an ecosystem, they can consume up to 90% of the plant biomass in an area. Herbivory can also change the types of plant species, allocation of root and leaf biomass, and impact nutrient cycling. However, lemming populations in the Arctic have been decline over the previous decades and some regions have seen lemmings disappear. A study done by Lara et al. in 2017 focused on better understanding the impact of lemming herbivory on the soil conditions and the carbon cycle in Arctic ecosystems. This research can help indicate the potential impacts that declines in lemming herbivory might have on changing Arctic ecosystems. Lara et al. looked at the impact of lemmings by using a long term fenced exclosure that kept lemmings out (pictured below) for more than 50 years. The authors found that there were sufficient lemming population outbreaks in frequent intervals to properly capture long term lemming impacts on the ecosystem. Example of a grazing exclosure by Hugh Venables, CC BY-SA 2.0, via Wikimedia Commons They measured many components of the carbon cycle including how much carbon dioxide was released from the soil versus taken in by plants. This is called net ecosystem exchange (NEE in g CO2 m–2 day–1) and it was measured in plots with exclosures (exclusion, lemmings cannot enter the plot, no herbivory) and without exclosures (control, lemmings can freely graze in the plot). Methane is another greenhouse gas that can be released by bacteria in Arctic soils typically when soils are flooded or hold a high volume of water. The vegetation at the soil surface can influence the release of methane from soils. Some plants allow methane to readily escape the soil. Plant water use and leaf also influences the amount of water in the soil. Methane emissions from Arctic soils are a major research focus of Arctic climate change research since changes in methane production in Arctic soils may amplify climate change. Other measurements also included factors that influence the carbon cycle such as soil temperature (SoilTemp_1, ° C). Higher soil temperature can lead to greater microbial activity leading to greater greenhouse gas emissions. The tutorial will focus on analyzing the impacts of lemming grazing of vegetation on methane fluxes. Here’s a look at the data: #file path on Rstudio cloud lemmings &lt;- read.csv(&quot;/cloud/project/activity04/lemming_JEco_HerbEx.csv&quot;) PLOT herbivory_trt CH4_Flux NEE SoilTemp_1 1 21C Ctl -5.708030 -0.072987056 10.6 2 22C Ctl 3.301541 0.475255463 15.6 3 23C Ctl -1.004405 0.485916744 10.0 4 24C Ctl 16.490522 -0.028536595 12.2 5 15C Ctl 17.189540 2.391257958 6.1 6 11C Ctl 15.736118 0.008257762 7.8 Here, there are three variables in this data, there are CH4 fluxes (CH4_Flux mgC m–2 day–1) and there is an herbivory treatment column (herbivory_trt) where a value of Ctl indicates that it was a control plot with no exclosure that was was open to lemming grazing. The Ex rows will indicate that an exclosure was applied preventing lemmings from accessing the site. The net ecosystem exchange column represents the amount of CO2 produced in the plot (NEE, mgC m–2 day–1). A negative value means that the plants are taking in more CO2 through photosynthesis than what gets released through respiration by plants and decomposition. It will be helpful to treat the herbivory column as a factor: #convert treatment data to factor lemmings$herbivory_trt &lt;- as.factor(lemmings$herbivory_trt) ggplot2 will be used plotting in this tutorial. To follow along, install the package and load it to the environment: library(ggplot2) A boxplot will help visualize the differences in methane fluxes between treatments. Here positive values indicate that methane is being emitted from the plot and a negative value indicates that there is methane uptake occurring over the plot surface. Let’s also use the stat_summary function in ggplot2 to add the mean to the plot too. This will really help with getting a sense for the range of data: ggplot(lemmings, aes(x=herbivory_trt, y=CH4_Flux))+ # data geom_boxplot()+ # box plot geometry stat_summary(geom=&quot;point&quot;, # add points fun = &quot;mean&quot;, # for the mean col=&quot;tomato3&quot;, # color (will be red points) size=4)+ # size of point labs(x=&quot;Herbivory treatment&quot;, y=&quot;Methane flux (mgC m –2 day–1)&quot;)+ # change the axis labels theme_classic() # plot with no gridlines The boxplot shows breaks the data up into quartiles (the values where we observe 0, 25, 50, 75,and 100% of the data). Here the thick black lines show the median of the data. This is the value where 50% of the data observations are above the value and below the value. The grey shaded box shows the range between 25-75%. The whiskers or lines show the rest of the range of data. Note that this function automatically separates potential outliers and shows them as open points. However, this is based on a calculation and these observations may be legitimate observations to include in our analysis. In this case, the observation is well within an observable and reasonable range, it just does not occur frequently in the exclusion plots. We have no reason to believe this data should be thrown out and will keep it in our analysis. Looking at the means for each group can also help contextualize the typical methane fluxes in each plot. The tapply function is similar to aggregate, and allows you to apply a function to an index. It returns a vector of output with each element labeled with the index value. meansCH4 &lt;- tapply(lemmings$CH4_Flux,#data lemmings$herbivory_trt,#index for each calculation &quot;mean&quot;) #function to use meansCH4 Ctl Ex 18.814645 5.714765 At first glance, the means seem to be different. The exclusion plot mean of 5.7 g C m–2 day–1 seems lower than the control plot mean of 18.8 g C m–2 day–1, but we need to account for the uncertainty around the mean. The t-test can help support our conclusions. 5.3.2 Study goals and statistical hypotheses With the help of statistical analysis, we can examine whether there is a difference in methane fluxes between the control plot (allows grazing) and the exclusion plots (prevents grazing) will help answer this question. The study can be guided by the over all question: Does lemming grazing impact methane fluxes? This question is open ended and could be answered with grazing increasing, decreasing, or having no influence on methane fluxes. The boxplot and means in the above section suggested there may be some differences in the data, but there is also a fair amount of overlap in the spread of the data. Statistical hypothesis testing can support making more robust conclusions about differences between treatments than simply looking at a graph. A two-sample t-test can be used to make this comparison. The test is structured to examine whether two population means differ. We can use this test to examine whether the control has a different mean methane flux than the exclosure treatment. In statistical hypothesis testing, there are often two hypothesis that the test examines called the null hypothesis and the alternative hypothesis. The statistical test examines whether the null hypothesis should be accepted or rejected (subsequently accepting the alternative hypothesis). Statistical hypothesis are separate from the scientific hypothesis and are structured around the specific assumptions and framework of the test. Here our hypothesis for the t-test would be H0: The means do not differ between the two groups. HA: There is a difference between the means of the two groups. 5.3.3 Checking assumptions of test The two sample t-test makes the following assumptions: 1. Independently and randomly sampled continuous data 2. Each group is normally distributed 3. Variances of the two group are equal Variance is another measure of the spread of data. It is part of the standard deviation calculation: the sum of the squared difference between each observation from the mean. 5.3.3.1 Assumption of independence and random sampling Random, independent sampling is achieved through proper experimental design. Given the published experimental design of the study by Lara et al., and the assumptions the authors state in their study, we can conclude this assumption is sufficiently met. 5.3.3.2 Assumption of Normality It can be difficult to assess normality visually especially for small sample size. Some data may be assumed to be normal based on underlying knowledge about the data. If you are unsure, the Shapiro-Wilk test can be used to test normality more quantitatively (for sample size &lt; 2,000). In R, the function for the Shapiro Wilk test is shapiro.test and the only argument is the vector of data that you want to test. Since each treatment group must be tested directly, we will subset the treatments directly in the function using the [] notation with the relational statement: #use the shapiro wilks test to look for normality in each treatment #shapiro-wilk test on grazing plots shapiro.test(lemmings$CH4_Flux[lemmings$herbivory_trt == &quot;Ctl&quot;]) Shapiro-Wilk normality test data: lemmings$CH4_Flux[lemmings$herbivory_trt == &quot;Ctl&quot;] W = 0.87763, p-value = 0.08173 The shapiro-wilk test is set up as a statistical test. This means that you are testing the null hypothesis: H0: the data are normally distributed. HA: the data are not normally distributed. You’ll use the conventional confidence level (\\(\\alpha\\) = 0.05) for our statistics in this class. In this test, we’ll rely on the p-value associated with the test statistic to assess how this outcome compares to our confidence level. A p-value of less than 0.05 would indicate that we should reject our null hypothesis since it would be unlikely to observe this test statistic value if the data were normally distributed. If the p-value is greater than 0.05, then we can assume that our data is normally distributed. You can see that the control plots do not deviate from a normal distribution. Next let’s test the exclusion plots: #shapiro-wilk test on grazing exclusion plots shapiro.test(lemmings$CH4_Flux[lemmings$herbivory_trt == &quot;Ex&quot;]) Shapiro-Wilk normality test data: lemmings$CH4_Flux[lemmings$herbivory_trt == &quot;Ex&quot;] W = 0.93325, p-value = 0.4158 5.3.3.3 Assumption of equal variance You can also test for differences in the variance between groups. The t-test can still be run if the variances are not equal, but will need a slightly different equation. In R, you use the bartlett.test function to run the test statistic. This test statistic has the following hypotheses: H0: Groups have similar variances HA: Groups do not have similar variances Here accepting the null would confirm the assumption that the variances do not differ between groups. A p-value of less than 0.05 would indicate we would be unlikely to observe the difference in variances under the null hypothesis. #use bartlett test since testing for equal variance bartlett.test(lemmings$CH4_Flux ~ lemmings$herbivory_trt) Bartlett test of homogeneity of variances data: lemmings$CH4_Flux by lemmings$herbivory_trt Bartlett&#39;s K-squared = 0.21236, df = 1, p-value = 0.6449 Here, the test statistic is well above 0.05 so you can assume the variances are equal and accept the null hypothesis. 5.3.4 Two sample t-test in R You can now conduct a two sample t-test now that you know the data meets all of the assumptions needed to reliably interpret the test. Now that all assumptions have been tested, we can revist the original statistical hypotheses: H0: The mean methane flux does not differ between the control and exclosure plots. HA: The mean methane flux does differ between the control and exclosure plots. The t-test statistic is calculated based on the difference between means. Here a two sample t-test (with equal variance) is calculated based on the difference between means, the sample size, and the pooled standard deviation. No difference between means would be interpreted as the mean for group 1 is equal to the mean for group 2. In R, the t.test function performs a t-test. Keep in mind this is a two sample t-test. Our data is formatted so that the identifier of the treatments is in the herbivory_trt column and the data for both groups is in the CH4_Flux column. The easiest way to tell R how to use this data is using a formula expression. Typically formulas follow this expression: Dependent variable ~ Independent variables. That symbol is called a tilde and the key for it is usually just above your tab key and next to the 1 key on your keyboard. Whenever, you see an expression you can think of it as dependent variable is a function of the independent variables. This is a common way to express statistical relationships and we will build on it with more complex relationships. Here we’ll use this formula notation to indicate that the t-test for CH4_Flux data should be a function of the groups indicated by the herbivory_trt column. t.test(lemmings$CH4_Flux ~ lemmings$herbivory_trt, var.equal=TRUE) Two Sample t-test data: lemmings$CH4_Flux by lemmings$herbivory_trt t = 1.5328, df = 22, p-value = 0.1396 alternative hypothesis: true difference in means between group Ctl and group Ex is not equal to 0 95 percent confidence interval: -4.624054 30.823815 sample estimates: mean in group Ctl mean in group Ex 18.814645 5.714765 There are many types of t-tests that can be run in R with slightly different assumptions. The var.equal argument specifies that this is a t-test with equal variance between groups. There are other types of t-test that can be run with this argument. Changing the argument to var.equal=FALSE would run a two-sample t-test with unequal variance (see lecture notes for equation). You can see that the p value for the t-test statistic is well above the confidence level of 0.05. The 95% confidence interval also overlaps with zero. This indicates that we cannot reject the null hypothesis. The difference between means is not statistically significant. Lemming grazing did not significantly alter methane fluxes. Keep in mind that either outcome of accepting or rejecting the null hypothesis is still of interest scientifically. Knowing that lemming grazing did not impact methane release still conveys information about greenhouse gases in Arctic ecosystems. Do not fall into the trap of thinking that accepting the null hypothesis is uninteresting or not relevant. It is still information! 5.4 One sample t-test in R The one sample t-test compares the mean of a vector of data to a mean specified by the null hypothesis. Let’s explore running a one sample t-test. Let’s test to see if the control plot methane levels are statistically different from zero (no methane produced). H0: The mean of the data variable does not differ from zero HA: The mean of the data variable is different from zero The one sample t-test still requires normally distributed data. We checked this group before using the shapiro test, but here is a reminder: #shapiro-wilk test on grazing exclusion plots shapiro.test(lemmings$CH4_Flux[lemmings$herbivory_trt == &quot;Ctl&quot;]) Shapiro-Wilk normality test data: lemmings$CH4_Flux[lemmings$herbivory_trt == &quot;Ctl&quot;] W = 0.87763, p-value = 0.08173 Now you can run the one sample t-test. The mu argument provides the mean value used in the null hypothesis (set to zero in our case): # one sided t test with the null mean set to zero t.test(lemmings$CH4_Flux[lemmings$herbivory_trt == &quot;Ctl&quot;], mu=0) One Sample t-test data: lemmings$CH4_Flux[lemmings$herbivory_trt == &quot;Ctl&quot;] t = 2.9142, df = 11, p-value = 0.01408 alternative hypothesis: true mean is not equal to 0 95 percent confidence interval: 4.60478 33.02451 sample estimates: mean of x 18.81465 The t-test shows the range in the 95% confidence interval does not overlap with zero (with the lower interval at least 4 mg C per day above zero) and the p value is below the confidence level. This is a strong indicator that the methane produced in the control plot does significantly differ from zero. This helps tell us that under typical tundra conditions that include lemming grazing methane production is not negligible with a mean well above zero. This helps add context that tundra ecosystems can be a source of methane under typical, untreated conditions that allow for natural herbivory and grazing by small rodents and lemmings. 5.5 Introduction to chi-squared test 5.5.1 The data The next test is useful if you want to collect categorical data (also known as nominal data). Categorical data fits into distinct predefined categories. For example, if you counted the number of blue and red cars driving down College Hill Road in an hour, you have data in two distinct categories: # of red cars and # of blue cars. Farnsworth (2004) looked at the influence of legal protections on populations of rare plant species. Here we have two variables each with two outcomes. The first describes legal protection with outcomes being protected or not protected. The second is the status of population: stable/increasing or decreasing. Here a decreasing observation for a species would indicate that the number of plants in that species is declining over time. You will set up our data in a contigency table. Rows and columns are assigned the outcomes for the two different variables and each cell of the table shows the number of observations in those categories. Each observation describes the number of species #set up contigency table species &lt;- matrix(c(18,8,15,32), ncol=2, byrow = TRUE) colnames(species) &lt;- c(&quot;Not.protected&quot;, &quot;Protected&quot;) rownames(species) &lt;- c(&quot;Declining&quot;, &quot;Stable.Increase&quot;) You can use a mosaic plot to visualize the proportion of the total observations in each category. You will see the size of the boxes is weighted by the proportion of species within each category. A larger box indicates that there are more observations in a category. #make a mosaic plot with an informative title and axes labels mosaicplot(species, col=c(&quot;tomato3&quot;, &quot;royalblue4&quot;), # add color to rows xlab=&quot;population status&quot;, # x label ylab=&quot;legal protection&quot;, # y label main=&quot;Legal protection impacts on populations&quot;) # title If legal protections don’t have any impact on populations, we would expect the species to be randomly distributed in each population status category and legal protection category. 5.5.2 Chi squared test Visually it looks like there may be some difference in population status that depends on legal protections. However, it is difficult to know whether this difference is significant. You can test whether the frequency of observations in each category meets the expectation under a null hypothesis. Here the statistical hypotheses are: H0: Observations in each category occur at expected frequencies HA: Observations in each category differ from expected frequencies The test statistic is calculated by comparing the observations to expectations: \\(\\chi\\)2 = \\(\\sum (O-E)^2/E\\) This statistic uses a \\(\\chi\\)2 distribution to evaluate how likely we are to observe deviations from the expected value under the null hypothesis. Higher deviations from expected values result in a greater \\(\\chi\\)2 statistic. The \\(\\chi\\)2 goodness of fit test in R uses the chisq.test function. By default the test will assume a null hypothesis with equal proportions of occurrences for each cell of the contingency table. You can alter this if your null hypothesis should have different expected frequencies. By default, R would enter expected frequencies based on diving the total number of items in our data frame evenly. Here, with four different combinations, the null frequencies would be each population x protection outcome occurring at a rate of 25%. However, this deviates a little from statistical recommendations. It is recommended that you calculate expected frequencies that would occur when there is no association between the categories using the row and column totals. This can help account for differences in the number of samples collected in each category. For example, summing up the protected species, there are 40 species observed. The not protected species only has 33 total observations. There are also less species experiencing declines in the population compared to those experiencing stable/increasing population. The chi-squared statistic might pick up differences in these sample size by each category rather than the association between legal protection and population status. Expected frequencies can be calculated by multiplying row and column sums and dividing by the total number of observations. You can also enter expected proportions using the p argument if you have a hypothesis about the null proportions under a scientific premise. In this case, we only are looking for whether legal protection is associated with population status. This means we will calculate expected values under the null: #total number of species counted tot &lt;- sum(species) # sum up rows of population status tot.decline &lt;- sum(species[1, ]) tot.increase &lt;- sum(species[2, ]) # sum up columns for protection tot.noProtect &lt;- sum(species[,1]) tot.protect &lt;- sum(species[, 2]) #calculate expected values: expectedFreq &lt;- matrix(c(tot.decline*tot.noProtect, # each cell multiplies sums tot.decline*tot.protect, tot.increase*tot.noProtect, tot.increase*tot.protect), byrow=TRUE, ncol=2)/tot # divide by total numer colnames(expectedFreq ) &lt;- c(&quot;Not.protected&quot;, &quot;Protected&quot;) rownames(expectedFreq ) &lt;- c(&quot;Declining&quot;, &quot;Stable.Increase&quot;) Let’s look at the mosaic plot to see the expected outcomes for the null hypothesis: there is no association between legal protection and population status: mosaicplot(expectedFreq, col=c(&quot;tomato3&quot;, &quot;royalblue4&quot;), xlab=&quot;population status&quot;, ylab=&quot;legal protection&quot;, main=&quot;Null hypothesis for no association&quot;) The chisq.test function expects our contingency table to be converted to a vector. The as.vector will turn the table into a vector. This process goes column by column. So in this case the Not.Protected column will be the first two numbers in the vector. The p argument allows you to specify the proportions that would occur under expected frequencies. These values must all add up to one. #Conduct a chi-squared test chisq.test(x=species, # actual data y=expectedFreq) # expected values under null Pearson&#39;s Chi-squared test with Yates&#39; continuity correction data: species X-squared = 7.9642, df = 1, p-value = 0.004771 Here the chi-squared test statistic has a p value well under 0.05. The null hypothesis can be rejected. Note there is a correction applied to 2x2 contigency tables (Yates’ continuity correction) since the degrees of freedom is small. The null hypothesis assumed that each category combination would occur with equal frequencies. This means that the differences in population status for each legal protection category occur at rates that differ from random chance. We can conclude that legal protection status is associated with a higher number of species that have stable/increasing populations than expected by random chance. Species in areas that are not legally protected status have a higher number of species with declining populations than expected by random chance alone. 5.6 Citations Lemming grazing &amp; surface fluxes Lara, M., D. Johnson, C. Andresen, R. Hollister, and C. Tweedie. (2017). Peak season carbon exchanges shifts from a sink to a source following 50+ years of herbivore exclusion in an Arctic tundra ecosystem. Journal of Ecology 105:122-131. Data citation: Lemming grazing &amp; surface fluxes Mark Lara. 2016. Long-term functional impact of Herbivore Exclusion. Arctic Data Center. doi:10.18739/A2CC0TT3H. Species protections Farnsworth, E.J. (2004). Patterns of plant invasions at sites with rare plant species throughout New England. Rhodora: 97-117. "],["introduction-to-linear-regression.html", "Chapter 6 Introduction to linear regression 6.1 Learning objectives 6.2 New functions &amp; syntax 6.3 Simple linear regression: Are beavers flooding the Arctic? 6.4 Multiple linear regression: What makes an early spring? 6.5 Panels of bivariate plots 6.6 Check for multi-collinearity 6.7 Run the regression 6.8 Interpret the regression 6.9 Citations", " Chapter 6 Introduction to linear regression by Heather Kropp for ENVST 206: Introduction to Environmental Data Hamilton College 6.1 Learning objectives Work with data collected from satellites to examine the impact of beavers on the tundra Use citizen science data examine the timing of spring leaf out Learn how to conduct a linear regression in R Interpret statistical output in R Use the ifelse and par function for formatting data and plots 6.2 New functions &amp; syntax par, ifelse,lm, rstandard, qqnorm, qqline, plot( ~ x1 + x2 + .. xn), par, ifelse 6.3 Simple linear regression: Are beavers flooding the Arctic? 6.3.1 The data Measures of surface water, vegetation, and land cover/use are commonly derived from satellite observations. Once these features are quantified from the imagery, these repeated observations over time can be used to examine changes in water and the land surface. Throughout the last couple decades, beavers have notably been moving outside of their typical habitats and colonizing the Arctic tundra. You will start by working with a data collected by Jones et al. using measurements of surface water area taken from satellite imagery and surveys of beaver dams. Beaver dams result in flooding around the bodies of water, and this increase in beaver populations may affect the hydrology and carbon cycling in the Arctic. Below you can see an example of how these measurements were collected. Example of Satellite imagery that can be used to measure and characterize lakes and surface water. Image credit: Contains modified Copernicus Sentinel data 2018, CC BY-SA 3.0 IGO, via Wikimedia Commons. Surface water can be seen as the black areas in the photographs. Above the red arrow indicates the presence of the beaver dam that appeared in 2012 and the subsequent flooding. Yellow arrows indicate the areas where the frozen ground (permafrost) is beginning to thaw due to the newly formed surface water coverage. Jones and his coauthors analyzed satellite imagery to identify the presence of beaver dams and changes in surface water in the Baldwin Peninsula Alaska. We will work with the data that examines the surface water area and the number of beaver dams over almost two decades in the tundra. datB &lt;- read.csv(&quot;/cloud/project/activity05/beaver_dam.csv&quot;) head(datB) year dams.n area.ha 1 2002 2 594 2 2007 6 610 3 2008 7 623 4 2009 10 600 5 2010 15 618 6 2011 31 625 Here, we are interested in evaluating whether the presence of beaver dams is increasing surface water in the tundra. You will want to focus on the dams.n column which contains the total number of beaver dams in the area and the total area of surface water in hectares: area.ha. Let’s start by taking a look at the data. Here you will make a scatter plot to visualize the data. You can use the plot function with to generate a scatter plot with x and y data. You will also notice that I am labeling the axes (with units!!!) and specifying that filled in points should be used. plot(datB$dams.n, datB$area.h, pch = 19, col = &quot;royalblue4&quot;, ylab = &quot;Surface water area (ha)&quot;, xlab = &quot;Number of beaver dams&quot;) 6.3.2 Set up the regression Just by looking at the scatter plot, the relationship looks fairly linear. The lm argument is used to specify a linear model. This is the function that will run the linear regression and provide all of the test information. Many of the linear regression assumptions are related to the residuals. You have to run the regression in order to look at the residuals. Note that this is a little different than previous hypothesis tests that required running the assumption checks before running the test. The rstandand function returns standardized residuals. #set up regression dam.mod &lt;- lm(datB$area.ha ~ datB$dams.n) #get standardized residuals dam.res &lt;- rstandard(dam.mod) 6.3.3 Checking assumptions Let’s assess all assumptions of the regression before interpreting results. First we’ll check normality of the residuals: #set up qq plot qqnorm(dam.res) #add qq line qqline(dam.res) You’ll notice that the points in the QQ plot mostly follow the line, but the last couple points deviate more from the line. You can also use the Shapiro-Wilks test if you are feeling unsure about whether this is significant enough to be non-normal. shapiro.test(dam.res) Shapiro-Wilk normality test data: dam.res W = 0.92993, p-value = 0.3793 Next let’s check the residual plot. The abline function adds a line to a graph, and can be useful for visualizing patterns in residuals. A line at a residual value of zero can be used to assess if there are any trends in the residuals. #make residual plot plot(datB$dams.n, dam.res, xlab = &quot;beaver damns&quot;, ylab = &quot;standardized residual&quot;) #add a horizontal line at zero abline(h=0) You can see that there are no major concerns about the regression assumptions around the residuals. You can go ahead and interpret the results. 6.3.4 Interpreting results You can use the summary function to print out the regression table. summary(dam.mod ) Call: lm(formula = datB$area.ha ~ datB$dams.n) Residuals: Min 1Q Median 3Q Max -12.639 -7.191 -1.006 6.954 14.772 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 606.00410 4.13835 146.436 &lt; 2e-16 *** datB$dams.n 0.31769 0.08037 3.953 0.00272 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 9.251 on 10 degrees of freedom Multiple R-squared: 0.6097, Adjusted R-squared: 0.5707 F-statistic: 15.62 on 1 and 10 DF, p-value: 0.002718 It can also be helpful to view your regression line on your plot. Putting the regression model in the abline function will automatically draw the regression line. #make plot of beaver dams and surface water plot(datB$dams.n, datB$area.h, pch = 19, col = &quot;royalblue4&quot;, ylab = &quot;Surface water area (ha)&quot;, xlab = &quot;Number of beaver dams&quot;) #add regression line #make line width thicker abline(dam.mod, lwd=2) 6.4 Multiple linear regression: What makes an early spring? 6.4.1 The data The time that leaves on deciduous trees unfurl from the leaf bud in the spring can impact forest temperatures, water cycling, and the overall productivity of forests. This unfurling of leaves in the spring is called leaf out. Warmer spring temperatures under climate change may result in deciduous plants becoming active earlier. The United States Geological Survey’s National Phenology Network collects data on the timing of plant activity and growth (aka phenology). The NPN relies on the work of plant scientists and interested volunteers, called citizen scientists. Many people can readily identify when leaf out is occurring, and dedicated citizen scientists help document where and when leaf out happens. This greatly increases the amount and spatial coverage of leaf out data beyond what research scientists are able to go out and observe themselves. Leaves beginning to unfurl from the leaf bud in the spring. Friedrich Haag / Wikimedia Commons / CC BY-SA 4.0 We will focus on the leaf out of the red maple, a tree common throughout the eastern United States. By U.S. Geological Survey - Digital representation of \"Atlas of United States Trees\" by Elbert L. Little, Jr. [1], Public Domain Read in the data: pheno &lt;- read.csv(&quot;/cloud/project/red_maple_pheno.csv&quot;) The data contains the day of year (doy) that leaf out occured and the year (year) of occurrence. Day of year counts the days in the year where January 1st is day of year 1 and December 31st (non-leap year) is day of year 365. There are also latitude (Lat) and longitude (Long) observations for each leaf out event. Latitude can be associated with differences in solar radiation and temperature. Longitude lower values of longitude will be further west in the continental US and higher values will represent more eastern areas up to the Atlantic coast. There are meteorological data that describes the general environmental conditions including the maximum temperature (Tmax, \\(^\\circ\\) C ), minimum temperature (Tmin, \\(^\\circ\\) C ), precipitation (Prcp, mm) for each location of measurement. There is also an elevation observation (elev, m). Higher elevations can be associated with colder temperatures and longer periods of snow cover. There is also a description of whether the site is urban or rural (siteDesc, “Urban” or “Rural”). The main variable of interest is the doy variable that describes when the leaf out occurred. Let’s take a look at how that is related to to variables like the maximum temperature and precipitation. 6.5 Panels of bivariate plots Sometimes it useful to view bivariate plots (two variables) side by side for a multiple regression. You can look at plots side by side using the par function. par specifies parameters around plotting and you can use arguments like mfrow to specify showing multiple plots in a panel. The mfrow argument needs a vector of two numbers for input. The first number should be the number of rows of plots in the panel and second is the number of columns of plots. #set up panel of plots with one row and two columns par(mfrow=c(1,2)) plot(pheno$Tmax,pheno$doy, pch = 19, col = &quot;royalblue4&quot;, ylab = &quot;Day of leaf out&quot;, xlab = &quot;Maximum temperature (C)&quot;) plot(pheno$Lat,pheno$doy, pch = 19, col = &quot;royalblue4&quot;, ylab = &quot;Day of leaf out&quot;, xlab = &quot;Latitude (degrees)&quot;) Rstudio will automatically default to using this par argument until you tell it to stop. You can either hit the broom button above the plot to clear your plotting area or you can also run the command: dev.off() 6.6 Check for multi-collinearity Temperature appears to have an impact on the timing of leaf out, but we will want to see if other variables such as precipitation or elevation of the site may impact the timing of leaf out. Let’s see if there will be any issues looking at multiple variables at once. Let’s check for correlations. We’ll do a visual assessment and look at the correlations. You can use the plot function to generate a series of covariance plots. Covariance plots set up a matrix of plots and compares all possible combinations of variables. Notice that there is a formula used here, but there is no dependent variable only many independent variables that could be used in the regression. plot( ~ pheno$Lat + pheno$Tmax+ pheno$Tmin +pheno$Prcp + pheno$elev + pheno$siteDesc) You’ll notice that there are no real patterns between temperature and precipitation variables. However, variables such as latitude and both temperature variables look to have a tight linear relationship. Just visually you can see that including both latitude and the maximum temperature would be problematic. This is not surprising given that solar radiation plays a major role in warming the earth’s surface, and the input of solar radiation varies with latitude. In this case, we will want choose the variable that best represents the process and makes the most sense. Here, we know that temperature directly impacts metabolic activity and the ability of plants to come out of dormancy. Any patterns with latitude would likely be reflecting differences in temperature across latitude, and temperature is known to directly impact leaf out. 6.7 Run the regression Let’s build a multiple regression that investigates the relationship between leaf out phenology and latitude, precipitation, longitude, and the urban/rural designation. You’ll notice the urban/rural designation is a character. You will want to code this as a zero and one for the regression. You can use the ifelse function to make this variable. ifelse uses three arguments: the first one is a logical statement to be evaluated, the second is a value to include in the vector if the statement is true, and the last is a value to include in the vector if statement is false. pheno$urID &lt;- ifelse(pheno$siteDesc == &quot;Urban&quot;,1,0) Note that this will be a binary variable in our regression.You will think a little bit more about how to interpret those results after you run the regression. The multiple regression set up in R will look similar to the simple linear regression: mlr &lt;- lm(pheno$doy ~ pheno$Lat + pheno$Prcp + pheno$Long + pheno$urID) Notice how instead of one variable there is now an added plus sign for each new variable. Now you can evaluate the assumptions just like the simple linear regression (assuming you already looked at multicollinearity before running the regression). Note that there are 1,244 observations in this data and the Shapiro Wilks test is not meant for data with more than 1,000 observations. You will want to use the qqnorm evaluation: #get residuals mlr.res &lt;- rstandard(mlr) #check normality qqnorm(mlr.res) qqline(mlr.res) You also will want to use the fitted values for your regression in your residual plot given that there are multiple covariates in this case. You can calculate the fitted values from the regression line for each observation using the fitted function: #get fitted values mlFitted &lt;- fitted(mlr) #make residual plot plot(mlFitted,mlr.res,pch=19, xlab=&quot;regression fitted values&quot;, ylab=&quot;residual&quot;) abline(h=0) Does it seem strange to you that there are many data points at the same x value in this graph? What about in the bivariate plots of temperature? If it does seem strange, good eye! We will cover these issues in the homework. 6.8 Interpret the regression Next let’s look at the regression table to evaluate the relationships with leaf out. summary(mlr) Call: lm(formula = pheno$doy ~ pheno$Lat + pheno$Prcp + pheno$Long + pheno$urID) Residuals: Min 1Q Median 3Q Max -53.943 -11.547 -1.155 10.346 52.796 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 76.780740 11.258812 6.820 1.65e-11 *** pheno$Lat 2.699841 0.213306 12.657 &lt; 2e-16 *** pheno$Prcp 0.023409 0.004544 5.152 3.16e-07 *** pheno$Long 1.107019 0.051204 21.620 &lt; 2e-16 *** pheno$urID -6.064702 1.480484 -4.096 4.56e-05 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 16.92 on 924 degrees of freedom (315 observations deleted due to missingness) Multiple R-squared: 0.6168, Adjusted R-squared: 0.6152 F-statistic: 371.9 on 4 and 924 DF, p-value: &lt; 2.2e-16 Let’s think a little more about how this urban/rural variable and how to interpret it in a regression. Keep in mind that urban built environments can have surfaces that are typically much warmer than surrounding natural vegetated environments. Dark paved surfaces and buildings absorb greater levels of solar radiation, resulting in higher temperatures in the immediate urban area. Remember urban sites will be marked with a one and rural areas are marked with a zero. Consider the interpretation of the slope coefficient x1 versus x0. Remember urban sites will be marked with a one and rural areas are marked with a zero. 6.9 Citations Beavers &amp; surface water Jones, Benjamin M., et al. “Increase in beaver dams controls surface water and thermokarst dynamics in an Arctic tundra region, Baldwin Peninsula, northwestern Alaska.” Environmental Research Letters 15.7 (2020): 075005. Red maple phenology USA-NPN National Coordinating Office. 2016. USA National Phenology Network data product development framework and data product catalog, v 1.1. USA-NPN Technical Series 2016-001. www.usanpn.org. "],["introduction-to-analysis-of-variance-anova.html", "Chapter 7 Introduction to Analysis of Variance (ANOVA) 7.1 Learning objectives 7.2 New functions &amp; syntax 7.3 Measuring biodiversity: the data 7.4 ANOVA overview 7.5 ANOVA in R 7.6 Citations", " Chapter 7 Introduction to Analysis of Variance (ANOVA) by Heather Kropp for ENVST 206: Introduction to Environmental Data Hamilton College 7.1 Learning objectives Work with data related to measures of biodiversity Learn to conduct an ANOVA in R Visualize and interpret ANOVA 7.2 New functions &amp; syntax aov, TukeyHSD 7.3 Measuring biodiversity: the data A decline in biodiversity is a common environmental concern. Biodiversity is a concept related to a wide range in the number of species (plant, fungi, and animal) and abundance of populations in a species. Anthropogenic impacts on the environment via habitat loss, increased global temperatures, and pollution can all negatively impact the abundance of living organisms. Extinction of organisms as a result of these impacts is decreasing biodiversity. There are a number of ways to measure biodiversity. Species richness is the number of different types of species present in a particular location. It is typically measured as a numerical count of species and collected from observations that look for presence/absence of different species. Diversity indices account for both the number of different types of species and the evenness of the number of individuals in each type of species. The urban environment can affect survival and reproduction of some insect species. Urban built environments are often warmer environments with less water availability, particularly in drier regions of California. Buildings and pavement can remove habitat and warmer temperatures on these surfaces may render them unsuitable for insects. Residential areas of urban environments can provide more vegetated environments, but plant cover is often not native and mostly dominated by a single plant cover such as grass. Some insects may be well equipped to survive in many urban environments and there is a concern that urbanization can drive declines in the diversity of insects. This may negatively impact essential part of ecosystems driving pollination, the food web, and the decomposition of plant material. To evaluate the impact of the urban environment on insects, Adams et al. (2019) trapped insects across the Los Angeles area and examined the number of species at each location. Below is a map of sampling locations used in the study across differing levels of urbanization. Aerial view of Los Angeles area highlights the varying levels of urbanization from the highly devleoped areas of commercial complexes and downtown to mountain parks Source: Tuxyso, CC BY-SA 3.0, via Wikimedia Commons #read in insect data datI &lt;- read.csv(&quot;/cloud/project/activity06/insect_richness.csv&quot;) urbanType urbanName Richness lawn 1 3 Suburban 30 Yes 2 8 Developed 3 Yes 3 9 Dense 22 No 4 9 Dense 1 Yes 5 8 Developed 22 No 6 8 Developed 15 No As you learned in class, ANOVA is a widely used technique for testing for differences in three or more groups. ANOVA will be useful for examining the insect species richness across an urban gradient in Los Angles. The Richness and the urbanName column will be used for the test. urbanName gives the type of urban environment the species richness was measured in. You will want to specify urbanName as a factor to work best in the ANOVA. The urbanType column provides an identifying number originally used in by the authors. We are only focusing on a few urban areas in this analysis. Below is a table that provides a more detailed description of each area used in the study site. The numbers in the table coincide with the urbanType column. _Table describing urban type classification from Adams et al. (2019) It will be helpful to convert the names to factors in R. datI$urbanName &lt;- as.factor(datI$urbanName) 7.4 ANOVA overview In ANOVA, the F statistic is used to evaluate differences in data between groups. Here the F-ratio that we use for our statistical evaluation is calculated based on the variance of the data around the mean. First the among group (SSamong, sometimes referred to as between) sum of squares calculates the squared difference between each group mean and the overall mean of all the data and sums up the differences for all groups. The within group (SSwithin, also referred to as residual) sum of squares calculates the squared difference between each data observation and its group mean and sums these differences for all observations. The mean squares is then calculated using the number of groups (SSamong) and both the number of groups and observations (SSwithin). This table also shows the total SS for good measure, but we don’t often look at it since SStotal = SSamong + SSwithin. The F-ratio is calculated by dividing the mean square for among groups by within groups. The p-value provides a probability for observing the calculated F-ratio and all values greater under the null distribution. You will want to use the standard confidence level of 0.05 to make your assessment. Source | Degrees of freedom | Sum of Squares | Mean square | F-ratio | P-value _______|____________________|________________|_____________|_________|________ Among groups | a-1 | \\(\\sum_{i=1}^{a} \\sum_{j=1}^{n}(\\overline{Y_i}-\\overline{Y})^{2}\\) | \\(\\frac{SS_{among}}{a-1}\\) | \\(\\frac{MS_{among}}{MS_{within}}\\) | p(F-distribution with (a-1) and a(n-1) degrees of freedom) Within groups | a(n-1) | \\(\\sum_{i=1}^{a} \\sum_{j=1}^{n}({Y_{ij}}-\\overline{Y_{i}})^{2}\\) | | | Here a represents the total number of groups and n represents the number of observations within groups. 7.5 ANOVA in R 7.5.1 Checking assumptions Next you will run the ANOVA test on the insect data to examine whether the different urban environments impact species richness. Before checking the assumptions, it’s a good idea to visualize the data. I’ll show the data using the plots from class that help visualize the different values used in ANOVA. The grey points show individual observations overlain on the boxplot for each group. The red points show the group mean and the blue line shows the mean for all the data. The first assumption is normality for each group. You must check normality for each individual group. Don’t forget that unique can be helpful for getting the names for each group: unique(datI$urbanName) [1] Suburban Developed Dense Natural Levels: Dense Developed Natural Suburban The sample size is small enough to use the Shapiro Wilk’s test. Shapiro-Wilk normality test data: datI$Richness[datI$urbanName == &quot;Suburban&quot;] W = 0.98087, p-value = 0.2461 Shapiro-Wilk normality test data: datI$Richness[datI$urbanName == &quot;Developed&quot;] W = 0.96155, p-value = 0.05604 Shapiro-Wilk normality test data: datI$Richness[datI$urbanName == &quot;Dense&quot;] W = 0.97508, p-value = 0.1025 Shapiro-Wilk normality test data: datI$Richness[datI$urbanName == &quot;Natural&quot;] W = 0.91559, p-value = 0.2514 The null hypothesis is that the data are normally distributed. Here, each group is higher than our confidencle level of 0.05. The second assumption of equal variance can be checked with the Bartlett test. bartlett.test(datI$Richness~datI$urbanName) Bartlett test of homogeneity of variances data: datI$Richness by datI$urbanName Bartlett&#39;s K-squared = 1.2091, df = 3, p-value = 0.7508 Both equal variance and normality assumptions are appropriate for the data. The other assumptions are that observations are independent of each other and correctly classified in each group. Since this data comes from a peer reviewed study that uses the same type of statistics (frequentist), we can assume that the research design and data are set up to meet these assumptions. 7.5.2 Running the ANOVA Now that you’ve had a chance to check the assumptions and visualize the data, you are ready to run the ANOVA in R. There are two steps to this test in R. You first specify a linear model with the lm function. The next step is to run the ANOVA calcluations using aov function. Both of these functions will simply run the calculations. To view the ANOVA table of results, you will print the table using the summary function. In the lm function we will specify a formula Richness ~ Urban. #specify model for species richness and urban type in.mod &lt;- lm(datI$Richness ~ datI$urbanName) #run the ANOVA in.aov &lt;- aov(in.mod) #print out ANOVA table summary(in.aov) Df Sum Sq Mean Sq F value Pr(&gt;F) datI$urbanName 3 1944 647.9 4.898 0.00254 ** Residuals 236 31216 132.3 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Here the table will print out such that the upper row labeled with our group vector name (datI$urbanName) is our among group variability (sum of squares for group means compared to overall mean) and the residuals row is the within group variability. You can see the F value in the upper right part of the table and the associated p-value. 7.5.3 Post hoc comparisons The ANOVA results alone do not give us a complete picture of the differences between urban landcover types. Some groups actually look quite similar. You must check to see how the means compare between groups to make a full statistical conclusion. You would want to highlight that only some groups may differ from each other when drawing conclusions about the urban environment impacts on species richness. This pairwise comparison between groups is called a post hoc test. Tukey’s HSD (Honestly Significant Difference) test can be used to conduct this comparison.There is a simple built in function in R. You just have to give the name of the variable that has the ANOVA results. #run Tukey HSD tukeyT &lt;- TukeyHSD(in.aov) #view results tukeyT Tukey multiple comparisons of means 95% family-wise confidence level Fit: aov(formula = in.mod) $`datI$urbanName` diff lwr upr p adj Developed-Dense 1.433333 -3.5966663 6.4633329 0.8819569 Natural-Dense 12.583333 3.3998525 21.7668141 0.0026479 Suburban-Dense 3.785714 -0.8060261 8.3774547 0.1456297 Natural-Developed 11.150000 1.7397324 20.5602676 0.0128693 Suburban-Developed 2.352381 -2.6776186 7.3823805 0.6210733 Suburban-Natural -8.797619 -17.9810999 0.3858618 0.0658910 Here you will notice that there are two group names in each row indicating which groups are being compared. A significant difference between groups means that the confidence interval in the difference between the means will not overlap with zero and will have a p-value below our confidence level threshold of 0.05. Note that this test prints a p-adj value. This means it has made some adjustments to account for the fact that were a comparing many means and should account for this in our calculations. You can also use the plot function and input the entire test variable to generate a plot that shows the confidence levels for the mean comparisons. This can help you get a better visualization for how close these intervals are to zero since there are a lot of numbers printed out in the table. #make a plot #make axes labels smaller than usual to fit on plot using cex.axis plot(tukeyT, cex.axis=0.75) 7.5.4 Interpreting the results When you interpret ANOVA results, you will want to summarize the test results, the means and difference between means, and make a plot that visualizes the data in each group. Typically a bar or box plot is used to show the means or quartiles of each group with letters above the data to indicate when groups are similar. For example, an ANOVA with four groups that all significantly differed from each other would show a different letter for each group like a,b,c,d.  The Tukey HSD results indicate that the Dense, Developed, and Suburban are all similar. Natural differs from Dense and Developed, but not Suburban. Multiple letters can be used for a group. Dense, Developed, and Suburban all get the letter a. Natural will get the letter b. Since Suburban is similar to Natural, it will also get a b. The best way to add labels to a base plot in R is the the text function. The arguments are text(x coordinates, y coordinates, text labels to add). In the code below, the coordinates and labels will be set up as vectors. # box plot plot(datI$Richness ~ datI$urbanName, xlab = &quot;Urban surface&quot;, ylab= &quot;Insect species Richness&quot;, ylim=c(0,60)) # x coordinates for labels xname &lt;- as.factor(c(&quot;Dense&quot;,&quot;Developed&quot;,&quot;Natural&quot;,&quot;Suburban&quot;)) # y coordinates for labels ylabel &lt;- c(50,55,50,55) # labels to add textName &lt;- c(&quot;a&quot;,&quot;a&quot;,&quot;b&quot;,&quot;a,b&quot;) # add letters to existing plot text(xname, #x coordinate ylabel, #y coordinate textName) # labels 7.6 Citations Insect Species Richness Adams, B., E. Li, C. Bahlai, E. Meineke, T. McGlynn, and B. Brown. (2020). Local- and landscape-scale variables shape insect diversity in an urban biodiversity hot spot. Ecological Applications. 30: e02089. Data citation Adams, Benjamin et al. (2020), Local and landscape scale variables shape insect diversity in an urban biodiversity hotspot., v2, Dryad, Dataset, https://doi.org/10.5061/dryad.7d7wm37rd output: html_document: theme: yeti highlight: pygments — "],["advanced-data-wrangling.-linking-soil-moisture-with-landcover-and-rainfall..html", "Chapter 8 Advanced data wrangling. Linking soil moisture with landcover and rainfall. 8.1 Learning objectives 8.2 New functions &amp; syntax 8.3 Soil moisture 8.4 Working with dates 8.5 Data wrangling with dplyr 8.6 Conclusions 8.7 Citations", " Chapter 8 Advanced data wrangling. Linking soil moisture with landcover and rainfall. by Heather Kropp for ENVST 206: Introduction to Environmental Data Hamilton College 8.1 Learning objectives Parse dates and extract timestamp information Filter and summarize data using tidy data functions Join data tables 8.2 New functions &amp; syntax ymd_hm, yday, hour, %&gt;%, left_join, right_join, full_join, inner_join 8.3 Soil moisture Soil is a porous layer composed of small particles of organic and mineral materials. The air spaces (pores) in between these particles hold water, roots, microbes, and invertebrates. Soil moisture refers to the amount of water stored in the pores surrounding soil particles. Soil moisture is a major component of the terrestrial water cycle. The water at the surface of the soil evaporates or is taken up by plant roots for transpiration. The total loss of water through these processes is referred to as evapotranspiration, adding water vapor to the atmosphere. source: USGS Data VizLab Soil moisture is essential for plant growth and survival. Too little soil moisture will limit photosynthesis as plants try to conserve water losses, reducing plant growth. Soil moisture also affects the activity of soil biota, affecting carbon and nutrient cycling in the soil. Excess soil moisture drains deeper into the ground resulting in recharge of deeper soil layers and ground water. Under saturated levels of soil moisture, all pores are filled with water. Until the excess water drains gravitationally, any additional rainfall will cause ponding at the surface or runoff. This can result in flooding. 8.3.1 Soil moisture in different types of land cover The soil moisture at a given point in time depends on the temperature and dryness of the air, the amount of precipitation, the amount of water plants are using, soil type, and the amount of leaf area that can intercept precipitation before it makes it to the ground. Many agricultural operations use irrigation to maintain enough soil moisture to encourage greater levels of plant growth. Flooding is a common issue around Clinton, NY. Mild droughts are also becoming more common in the northeastern United States under climate change, leading to plant stress, limitations in plant growth, and vulnerability to pests and disease. Soil moisture is the critical link between precipitation and conditions associated with flooding and plant stress. Land cover describes the characteristics of the land surface and vegetative cover. Differences in leaf area and transpiration in different types of vegetative land cover can affect soil moisture. We’ll look at soil moisture data in the upper 12 cm of the soil collected in three different types of land cover on Hamilton’s campus: a fallow agricultural field (fallow corn field near Rogers estate), a deciduous leaf forest (beech-maple forest in Rogers glen), and a mowed lawn (turf grass near Ferguson Parking lot). 8.3.2 The data: soil moisture The soil_moisture.csv file contains hourly soil moisture for each location. #read in soil moisture data soil &lt;- read.csv(&quot;/cloud/project/activity07/soil_moisture.csv&quot;) location doy hour soil.moist 1 ag field 190 0 0.3469002 2 ag field 190 1 0.3456291 3 ag field 190 2 0.3443037 4 ag field 190 3 0.3434466 5 ag field 190 4 0.3423160 6 ag field 190 5 0.3412638 The location column labels the type of land cover at the location of measurement. doy contains the day of year and hour is the hour in the day (0- 23) that the data was collected. The soil.moist column contains the volumetric soil moisture (cm 3 water cm -3 soil). The campus.csv contains data from the campus weather station. You will use the na.strings argument to account formatting of the weather station data logger. Missing data is stored as #N/A rather than NA. The na.strings argument will convert these characters to NA in R. #read in soil moisture data weather &lt;- read.csv(&quot;/cloud/project/activity07/campus.csv&quot;, na.strings= &quot;#N/A&quot;) Timestamps Solar.Rad.W.m2 Precipitation.mm wind.m.s air.temp.c 1 2022-05-31 19:00:00 122.4 0 0.64 27.8 2 2022-05-31 19:15:00 65.7 0 0.77 27.2 3 2022-05-31 19:30:00 47.1 0 0.58 26.7 4 2022-05-31 19:45:00 33.9 0 0.62 25.9 5 2022-05-31 20:00:00 21.1 0 0.57 25.0 6 2022-05-31 20:15:00 12.7 0 0.54 24.2 The weather data is collected every 15 minutes. The Timestamps columns contains the date and time information. Solar.Rad.W.m2 column contains the solar radiation (W m-2), Precipitation.mm contains the precipitation data. We will only look at data in the summer of 2022. During the summer, the only form of precipitation is rainfall (no hail or snow). wind.m.s contains the average wind speed (m s-1). air.temp.c is the air temperature (degrees C). 8.4 Working with dates 8.4.1 The lubridate package Lubridate makes working with dates easier than base R. Let’s install and load the package: install.packages(&quot;lubridate&quot;) library(lubridate) https://github.com/rstudio/cheatsheets/blob/main/lubridate.pdf 8.4.2 Parsing date/time In base R, you had to specify the date formatting with as.Date to convert the vector from characters to date. lubridate makes standardizing dates easier with the function that is named based on the order of your date formatting where y=year, m= month, d = day, h = hour, m=minute, s. The function works by combing the date and time elements that are in your date data. If both are in the same column then you separate the date and time letters with an underscore. You can set the timezone with the argument tz exampleDate &lt;- c(&quot;2021-01-10 05:23:30&quot;) #parse date with year, month, day hour:minute:second ymd_hms(exampleDate) [1] &quot;2021-01-10 05:23:30 UTC&quot; #parse date with timezone so that it is always in NY time #will account for daylight savings. Time local to NY ymd_hms(exampleDate, tz=&quot;America/New_York&quot;) [1] &quot;2021-01-10 05:23:30 EST&quot; #eastern standard time (note this doesn&#39;t work during daylight savings) ymd_hms(exampleDate, tz=&quot;EST&quot;) [1] &quot;2021-01-10 05:23:30 EST&quot; You can reformat the date in a weather data Timestamps column: weather$date &lt;- ymd_hms(weather$Timestamps, tz=&quot;America/New_York&quot;) 8.4.3 Extracting information from dates There are many functions in lubridate to extract information for dates. You can use the year function to extract just the year from the date. The only argument is the vector of dates: year(weather$date) You can get the day of year with yday: weather$doy &lt;- yday(weather$date) You can also add the hour the measurement was taken weather$hour &lt;- hour(weather$date) You will notice the soil moisture had the day of year and hour for information about the time/date. You now have this information for the campus weather. Note that there are four observations for every hour since the data is taken every hour. 8.5 Data wrangling with dplyr The dplyr package contains a number of helpful functions for data wrangling. install.packages(&quot;dplyr&quot;) library(dplyr) You’ll want to compare the precipitation data to the soil moisture data. That will allow you to examine increases in soil moisture due to rainfall. Let’s sum up the total precipitation that occurred in the hour rather than using the individual 15 minute increments. We will want to apply to sum function to each unique doy and hour function. This means that we want to treat doy and hour as grouping variables. A group indicates that each unique value in the group describes a homogeneous feature (e.g. landcover, day of year). Use the group_by function to specify a grouping variable and sum the preciptation and count the number of observations:. weatherHour &lt;- weather %&gt;% # start with weather group_by(doy, hour) %&gt;% # doy and hour are groups summarise(precip.mm = sum(Precipitation.mm, na.rm=TRUE),# sum precipitation into a new column called precip.mm n.precip = length(na.omit(Precipitation.mm)) ) # number of observations used to calculate precip.mm with NA omitted `summarise()` has grouped output by &#39;doy&#39;. You can override using the `.groups` argument. # A tibble: 6 × 4 # Groups: doy [2] doy hour precip.mm n.precip &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; 1 151 19 0 4 2 151 20 0 4 3 151 21 0 4 4 151 22 0 4 5 151 23 0 4 6 152 0 0 4 Note that you get a warning about the output. This is a standard warning to remind you that the default settings were used. You’ll notice the output is a little different. Tidyverse contains a data structure called a tibble A tibble is a data frame that makes it easier to access the information associated with a data frame including any relevant groups (see doy and hour listed) and the type of data in each column. 8.5.1 Filtering data You can use the filter function to subset data just like you did with the [] bracket notation. You use the same relational and logical statements covered in activity 2. Here’s a refresher: Operator | Interpretation | Type ———|—————-|—– == | equal to | relational != | not equal to | relational &gt; | more than (not including) | relational &gt;= | more than or equal to | relational &lt; | less than (not including) | relational &lt;= | less than or equal to | relational &amp; | and | logical | | or | logical Let’s remove any hours that don’t have all four observations present: weatherData &lt;- weatherHour %&gt;% # refer to weatherHour data frame filter(n.precip == 4) # only keep rows with exactly 4 observations in the hour # rows in new data frame nrow(weatherData) [1] 3358 # rows in old data frame nrow(weatherHour) [1] 3365 8.5.2 Joining data tables Your goal is to compare the precipitation and soil moisture data. It is clear there are slightly different time periods in each data frame. It would be helpful to match the data frames into a single data frame so that each row contains the same day of year and hour of observation. A join combines two tables based on one or more columns that contain the same identifying variables. We refer to the two tables as a left (table listed first) and the right table (second table). There are four different types of joins. Let’s explore the types of joins by combining the soil moisture and precipitation observations. A left join preserves the structure of the left table (left_join. Any data present in the left table are kept. Columns from the right table are matched to the identifying columns in the left table. Any data present in the left table, but not in the right table results in missing data in the new columns. Data are present in the right table but not in the left are not added in the join. The right join follows the same rules (right_join), preserving the structure of the right table. You can do a join with soil moisture as the left table and the hourly precipitation weather data as the right table. Each function is named by the type of join. soilLeftJoin &lt;- left_join(soil, # left table weatherData, # right table by=c(&quot;doy&quot;,&quot;hour&quot;)) #identifier columns [1] &quot;number of rows:&quot; [1] 1512 location doy hour soil.moist precip.mm n.precip 1 ag field 190 0 0.3469002 0 4 2 ag field 190 1 0.3456291 0 4 3 ag field 190 2 0.3443037 0 4 4 ag field 190 3 0.3434466 0 4 5 ag field 190 4 0.3423160 0 4 6 ag field 190 5 0.3412638 0 4 soilRightJoin &lt;- right_join(soil, # left table weatherData, # right table by=c(&quot;doy&quot;,&quot;hour&quot;)) #identifier columns [1] &quot;number of rows:&quot; [1] 4366 location doy hour soil.moist precip.mm n.precip 1 ag field 190 0 0.3469002 0 4 2 ag field 190 1 0.3456291 0 4 3 ag field 190 2 0.3443037 0 4 4 ag field 190 3 0.3434466 0 4 5 ag field 190 4 0.3423160 0 4 6 ag field 190 5 0.3412638 0 4 A full join keeps all observations in the join. If data are present in one table, but not the other, missing values are added. soilFullJoin &lt;- full_join(soil, # left table weatherData, # right table by=c(&quot;doy&quot;,&quot;hour&quot;)) #identifier columns [1] &quot;number of rows:&quot; [1] 4366 location doy hour soil.moist precip.mm n.precip 1 ag field 190 0 0.3469002 0 4 2 ag field 190 1 0.3456291 0 4 3 ag field 190 2 0.3443037 0 4 4 ag field 190 3 0.3434466 0 4 5 ag field 190 4 0.3423160 0 4 6 ag field 190 5 0.3412638 0 4 A inner join only keeps observations present in both tables. soilInnerJoin &lt;- inner_join(soil, # left table weatherData, # right table by=c(&quot;doy&quot;,&quot;hour&quot;)) #identifier columns [1] &quot;number of rows:&quot; [1] 1512 location doy hour soil.moist precip.mm n.precip 1 ag field 190 0 0.3469002 0 4 2 ag field 190 1 0.3456291 0 4 3 ag field 190 2 0.3443037 0 4 4 ag field 190 3 0.3434466 0 4 5 ag field 190 4 0.3423160 0 4 6 ag field 190 5 0.3412638 0 4 8.6 Conclusions You can now look at the data side by side. Let’s combine everything we have learned and look at the soil moisture on the hour with the most precipitation. For example, you can compare soil moisture at the hour with the most precipitation. # soil moisture on the day and hour with the most precipitation: maxSoil &lt;- soilInnerJoin %&gt;% # use joind data filter(precip.mm == max(precip.mm)) # look at hour and day with the highest precipitation maxSoil location doy hour soil.moist precip.mm n.precip 1 ag field 199 11 0.4348943 6.221 4 2 deciduous 199 11 0.3212613 6.221 4 3 mowed lawn 199 11 0.3097192 6.221 4 You can also look at a time series of each data value: #calculate a decimal doy for plotting soilInnerJoin$TIME = soilInnerJoin$doy+(soilInnerJoin$hour/24) ggplot(soilInnerJoin, aes(x=TIME, y=soil.moist, color=location))+ geom_point()+ geom_line()+ theme(legend.position=&quot;top&quot;)+ labs(x=&quot;Day of Year&quot;, y=&quot;Soil moisture (cm3, cm-3)&quot;) ggplot(soilInnerJoin, aes(x=TIME, y=precip.mm))+ geom_col()+ labs(x=&quot;Day of Year&quot;, y=&quot;Rainfall (mm)&quot;) You can see that the fallow agricultural field and mowed lawn have the greatest increases in soil moisture after rainfall. The mowed lawn also has the greatest dry down in between rainfall events. Using these data wrangling skills, you will continue to explore patterns in soil moisture in activity 7. 8.7 Citations Water Cycle Diagram. 2022. USGS Data VizLab. https://www.usgs.gov/special-topics/water-science-school/science/water-cycle-diagrams "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
